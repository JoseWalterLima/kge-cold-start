{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b415d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\José Walter\\Mestrado\\3. TrabalhoFinal\\GraphEmbedding\\app\\kgenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import random\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "# Default path to data files\n",
    "PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5451fa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0      196      242       1\n",
       "1      186      302       1\n",
       "2       22      377       1\n",
       "3      244       51       1\n",
       "4      166      346       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load user-item interaction data\n",
    "interaction_data = pd.read_csv(\n",
    "    PATH + 'ml-100k/u.data',\n",
    "    sep='\\t',\n",
    "    encoding=\"latin1\",\n",
    "    names=['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    )[['user_id', 'item_id', 'rating']]\n",
    "interaction_data['rating'] = 1\n",
    "display(interaction_data.shape)\n",
    "interaction_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ada4bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items on test set: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['159', '458', '679', '128', '658']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load test item IDs from the json file saved\n",
    "# previously from Knowledge Graph Method\n",
    "with open('../experiments/test_ids.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# Extract test item IDs as integers\n",
    "test_item_ids = [item['movieId'] for item in data]\n",
    "print(f\"items on test set: {len(test_item_ids)}\")\n",
    "display(test_item_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2d6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split interaction data into train and test sets\n",
    "# to allow later cold start evaluation \n",
    "train_interactions = interaction_data[\n",
    "    ~interaction_data['item_id'].astype(str).isin(test_item_ids)].reset_index(drop=True)\n",
    "test_interactions = interaction_data[\n",
    "    interaction_data['item_id'].astype(str).isin(test_item_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cff6db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=943, n_train_items=1652\n"
     ]
    }
   ],
   "source": [
    "# garantir que ids sejam strings para comparação segura\n",
    "train_interactions['user_id'] = train_interactions['user_id'].astype(str)\n",
    "train_interactions['item_id'] = train_interactions['item_id'].astype(str)\n",
    "test_item_ids = [str(x) for x in test_item_ids]  # já definido antes\n",
    "\n",
    "# criar lista ordenada de users observados no treino e items que permaneceram no grafo de treino\n",
    "unique_user_ids = sorted(train_interactions['user_id'].unique().tolist())\n",
    "train_item_ids = sorted(train_interactions['item_id'].unique().tolist())  # exclui itens de teste\n",
    "\n",
    "# construir mapas id -> índice\n",
    "user2idx = {u: i for i, u in enumerate(unique_user_ids)}\n",
    "item2idx = {v: i for i, v in enumerate(train_item_ids)}\n",
    "\n",
    "# contar e salvar (opcional, mas útil)\n",
    "n_users = len(user2idx)\n",
    "n_train_items = len(item2idx)\n",
    "print(f\"n_users={n_users}, n_train_items={n_train_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4933c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_train shape: torch.Size([2, 191922])\n",
      "num_edges (directed count): 191922\n",
      "num_unique_nodes referenced: 2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\José Walter Lima\\AppData\\Local\\Temp\\ipykernel_14376\\74960458.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  edge_index_train = torch.tensor([src, dst], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Mapear colunas para índices locais\n",
    "train_df = train_interactions.copy()\n",
    "train_df['u_idx'] = train_df['user_id'].map(user2idx)\n",
    "train_df['i_idx_local'] = train_df['item_id'].map(item2idx)  # items de treino apenas\n",
    "\n",
    "# Remover linhas sem mapeamento (segurança)\n",
    "train_df = train_df.dropna(subset=['u_idx','i_idx_local']).astype({'u_idx':int, 'i_idx_local':int})\n",
    "\n",
    "# Construir índices globais: shift nos items para evitar sobreposição com users\n",
    "u_nodes = train_df['u_idx'].values                      # já 0..n_users-1\n",
    "i_nodes = train_df['i_idx_local'].values + n_users     # items -> n_users .. n_users + n_train_items - 1\n",
    "\n",
    "# Duplicar para grafo não direcionado: user->item e item->user\n",
    "src = np.concatenate([u_nodes, i_nodes])\n",
    "dst = np.concatenate([i_nodes, u_nodes])\n",
    "\n",
    "edge_index_train = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "# Sanity checks e salvar\n",
    "print(\"edge_index_train shape:\", edge_index_train.shape)\n",
    "print(\"num_edges (directed count):\", edge_index_train.shape[1])\n",
    "print(\"num_unique_nodes referenced:\", int(torch.unique(edge_index_train).numel()))\n",
    "# salvar para uso posterior\n",
    "torch.save(edge_index_train, \"edge_index_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4212c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', ['age:24', 'gender:M', 'occupation:technician', 'zipcode:85'])\n",
      "('2', ['age:53', 'gender:F', 'occupation:other', 'zipcode:94'])\n",
      "('3', ['age:23', 'gender:M', 'occupation:writer', 'zipcode:32'])\n",
      "('4', ['age:24', 'gender:M', 'occupation:technician', 'zipcode:43'])\n",
      "('5', ['age:33', 'gender:F', 'occupation:other', 'zipcode:15'])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a defaultdict to hold user features\n",
    "user_data = defaultdict(dict)\n",
    "\n",
    "# Read data and build user features dictionary\n",
    "def load_feature(file_path, feature_name):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            userId = row['userId']\n",
    "            value = row[feature_name]\n",
    "            user_data[userId][feature_name] = value\n",
    "\n",
    "# Load each feature file\n",
    "load_feature(PATH + 'ageRel.csv', 'age')\n",
    "load_feature(PATH + 'genderRel.csv', 'gender')\n",
    "load_feature(PATH + 'occupationRel.csv', 'occupation')\n",
    "load_feature(PATH + 'residesRel.csv', 'zipcode')\n",
    "\n",
    "# Build user features as a dictionary (user_id -> list of \"k:v\" features)\n",
    "user_features_raw = {\n",
    "    str(userId): [\n",
    "        f'age:{data.get(\"age\",\"\")}',\n",
    "        f'gender:{data.get(\"gender\",\"\")}',\n",
    "        f'occupation:{data.get(\"occupation\",\"\")}',\n",
    "        f'zipcode:{data.get(\"zipcode\",\"\")}'\n",
    "    ]\n",
    "    for userId, data in user_data.items()\n",
    "}\n",
    "\n",
    "# Display first 5 user features\n",
    "for item in list(user_features_raw.items())[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba714d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2', ['releaseDate:Jan-1995', 'genre:Action', 'genre:Adventure', 'genre:Thriller'])\n",
      "('4', ['releaseDate:Jan-1995', 'genre:Action', 'genre:Comedy', 'genre:Drama'])\n",
      "('17', ['releaseDate:Feb-1996', 'genre:Action', 'genre:Comedy', 'genre:Crime', 'genre:Horror', 'genre:Thriller'])\n",
      "('21', ['releaseDate:Feb-1996', 'genre:Action', 'genre:Adventure', 'genre:Comedy', 'genre:Musical', 'genre:Thriller'])\n",
      "('22', ['releaseDate:Feb-1996', 'genre:Action', 'genre:Drama', 'genre:War'])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a defaultdict to hold item features\n",
    "item_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Read data and build item features dictionary\n",
    "# Modified version to handle multiple genres\n",
    "def load_feature(file_path, feature_name):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            itemId = row['movieId']\n",
    "            value = row[feature_name]\n",
    "            if feature_name == 'genreDesc':\n",
    "                item_data[itemId]['genre'].append(value)\n",
    "            else:\n",
    "                item_data[itemId][feature_name] = value\n",
    "\n",
    "# Load each feature file\n",
    "load_feature(PATH + 'releaseRel.csv', 'releaseDate')\n",
    "load_feature(PATH + 'genreRel.csv', 'genreDesc')\n",
    "\n",
    "# Build item features as a dictionary (item_id -> list of \"k:v\" features)\n",
    "item_features_raw = {\n",
    "    str(itemId): (\n",
    "        [f'releaseDate:{data.get(\"releaseDate\",\"\")}'] +\n",
    "        [f'genre:{genre}' for genre in data.get('genre', [])]\n",
    "    )\n",
    "    for itemId, data in item_data.items()\n",
    "}\n",
    "\n",
    "# Display first 5 item features\n",
    "for item in list(item_features_raw.items())[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb2b5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper para parse \"k:v\" lists\n",
    "def parse_kv_list(kv_list):\n",
    "    d = defaultdict(list)\n",
    "    for kv in kv_list:\n",
    "        if isinstance(kv, str) and \":\" in kv:\n",
    "            k,v = kv.split(\":\",1)\n",
    "            d[k].append(v)\n",
    "    # return simplified: single-value -> string, multi -> list\n",
    "    out = {}\n",
    "    for k,vals in d.items():\n",
    "        out[k] = vals[0] if len(vals)==1 else vals\n",
    "    return out\n",
    "\n",
    "# garantir chaves strings\n",
    "user_keys = list(user2idx.keys())        # ordem já definida antes\n",
    "train_item_keys = list(item2idx.keys())  # itens que estão no grafo de treino\n",
    "\n",
    "# criar parsed lists (preserve order)\n",
    "users_parsed = []\n",
    "for uid in user_keys:\n",
    "    kvs = user_features_raw.get(str(uid), [])   # ajuste: metadata map may use string keys\n",
    "    parsed = parse_kv_list(kvs)\n",
    "    parsed['id'] = uid\n",
    "    users_parsed.append(parsed)\n",
    "df_users = pd.DataFrame(users_parsed).set_index('id').fillna(\"\")\n",
    "\n",
    "items_parsed = []\n",
    "for iid in train_item_keys:\n",
    "    kvs = item_features_raw.get(str(iid), [])\n",
    "    parsed = parse_kv_list(kvs)\n",
    "    parsed['id'] = iid\n",
    "    items_parsed.append(parsed)\n",
    "df_items = pd.DataFrame(items_parsed).set_index('id').fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409b6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users: 943\n",
      "user_age shape: (943, 1)\n",
      "user_gender sample: ['M' 'M' 'M']\n",
      "user_occupation sample: ['technician' 'lawyer' 'executive']\n",
      "user_zipcode sample: ['85' '90' '90']\n",
      "n_train_items: 1652\n",
      "item_release sample: ['Jan-1995', 'Feb-1997', 'Jan-1994']\n",
      "item_genres sample: [['Animation', 'Childrens', 'Comedy'], ['Crime', 'Drama', 'Thriller'], ['Comedy', 'Western']]\n"
     ]
    }
   ],
   "source": [
    "# USERS\n",
    "# age numeric (fallback 0), gender string, occupation string, zipcode string\n",
    "user_age = df_users['age'].astype(float).values.reshape(-1,1) if 'age' in df_users.columns else np.zeros((len(df_users),1))\n",
    "user_gender = df_users['gender'].astype(str).values.reshape(-1,1) if 'gender' in df_users.columns else np.array([[\"\"]] * len(df_users))\n",
    "user_occupation = df_users['occupation'].astype(str).values.reshape(-1,1) if 'occupation' in df_users.columns else np.array([[\"\"]] * len(df_users))\n",
    "user_zipcode = df_users['zipcode'].astype(str).values.reshape(-1,1) if 'zipcode' in df_users.columns else np.array([[\"\"]] * len(df_users))\n",
    "\n",
    "# ITEMS\n",
    "# releaseDate kept as original string (e.g., \"Jan-1995\")\n",
    "# genres: ensure list for each item\n",
    "item_release_str = []\n",
    "item_genres_list = []\n",
    "for iid in df_items.index:\n",
    "    rd = df_items.loc[iid].get('releaseDate', \"\")\n",
    "    # keep original string (or empty string if missing)\n",
    "    item_release_str.append(rd if isinstance(rd, str) else \"\")\n",
    "    # genres: parsed could be list or single string\n",
    "    genres = df_items.loc[iid].get('genre', [])\n",
    "    if isinstance(genres, str):\n",
    "        genres = [genres] if genres else []\n",
    "    item_genres_list.append(genres)\n",
    "\n",
    "# Converter para arrays numpy apropriados\n",
    "# user_age already shaped (n_users,1)\n",
    "# user_gender, user_occupation, user_zipcode shaped (n_users,1)\n",
    "user_cat_inputs = {\n",
    "    'gender': user_gender,\n",
    "    'occupation': user_occupation,\n",
    "    'zipcode': user_zipcode\n",
    "}\n",
    "\n",
    "# item_release_str is a list of strings length n_items\n",
    "# item_genres_list is a list of lists length n_items\n",
    "\n",
    "# Mostre formas e amostras para verificação rápida\n",
    "print(\"n_users:\", len(df_users))\n",
    "print(\"user_age shape:\", user_age.shape)\n",
    "print(\"user_gender sample:\", user_gender[:3].ravel())\n",
    "print(\"user_occupation sample:\", user_occupation[:3].ravel())\n",
    "print(\"user_zipcode sample:\", user_zipcode[:3].ravel())\n",
    "\n",
    "print(\"n_train_items:\", len(df_items))\n",
    "print(\"item_release sample:\", item_release_str[:3])\n",
    "print(\"item_genres sample:\", item_genres_list[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0d63f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_cat_ohe shape: (943, 134)\n",
      "user_age_scaled shape: (943, 1)\n",
      "item_release_ohe shape: (1652, 109)\n",
      "genres_mx shape: (1652, 18)\n"
     ]
    }
   ],
   "source": [
    "# USERS\n",
    "# ohe_user: gender, occupation, zipcode combinados\n",
    "ohe_user = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "user_cat_input = np.hstack([user_cat_inputs['gender'], user_cat_inputs['occupation'], user_cat_inputs['zipcode']])  # shape (n_users, 3)\n",
    "user_cat_ohe = ohe_user.fit_transform(user_cat_input)  # (n_users, D_user_cat)\n",
    "\n",
    "# age scaler\n",
    "sc_user_age = StandardScaler().fit(user_age)  # user_age shape (n_users,1)\n",
    "user_age_scaled = sc_user_age.transform(user_age)  # (n_users,1)\n",
    "\n",
    "# ITEMS\n",
    "# releaseDate as categorical token -> OneHotEncoder\n",
    "ohe_release = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "item_release_arr = np.array(item_release_str).reshape(-1,1)  # (n_items,1)\n",
    "item_release_ohe = ohe_release.fit_transform(item_release_arr)  # (n_items, D_release)\n",
    "\n",
    "# genres multi-hot\n",
    "mlb_genres = MultiLabelBinarizer(sparse_output=False)\n",
    "genres_mx = mlb_genres.fit_transform(item_genres_list)  # (n_items, n_genres)\n",
    "\n",
    "# salvar encoders\n",
    "joblib.dump(ohe_user, \"ohe_user.joblib\")\n",
    "joblib.dump(sc_user_age, \"sc_user_age.joblib\")\n",
    "joblib.dump(ohe_release, \"ohe_release.joblib\")\n",
    "joblib.dump(mlb_genres, \"mlb_genres.joblib\")\n",
    "\n",
    "# prints para verificação\n",
    "print(\"user_cat_ohe shape:\", user_cat_ohe.shape)\n",
    "print(\"user_age_scaled shape:\", user_age_scaled.shape)\n",
    "print(\"item_release_ohe shape:\", item_release_ohe.shape)\n",
    "print(\"genres_mx shape:\", genres_mx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eaa2275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user sub-dim: 135\n",
      "item sub-dim: 127\n",
      "Final feature dimension D = 262\n",
      "Saved X_all_train.pt  shape: torch.Size([2595, 262])\n",
      "user_full shape: (943, 262)\n",
      "item_full shape: (1652, 262)\n"
     ]
    }
   ],
   "source": [
    "# Construir matrizes finais e concatenar (users then train items), salvar artefatos\n",
    "# verificar shapes\n",
    "user_feat_dim = user_age_scaled.shape[1] + user_cat_ohe.shape[1]   # ou user_features.shape[1] se já tiver\n",
    "item_feat_dim = item_release_ohe.shape[1] + genres_mx.shape[1]    # ou item_features.shape[1]\n",
    "\n",
    "print(\"user sub-dim:\", user_feat_dim)\n",
    "print(\"item sub-dim:\", item_feat_dim)\n",
    "\n",
    "# construir matrizes finais separadas (recalcular para evitar confusão)\n",
    "user_features = np.hstack([user_age_scaled, user_cat_ohe]).astype(float)    # shape (n_users, D_u)\n",
    "item_features = np.hstack([item_release_ohe, genres_mx]).astype(float)     # shape (n_items_train, D_i)\n",
    "\n",
    "n_users = user_features.shape[0]\n",
    "n_items_train = item_features.shape[0]\n",
    "\n",
    "# Dimensão comum: concatenação das sub-dimensões (users first, items second)\n",
    "D_user = user_features.shape[1]\n",
    "D_item = item_features.shape[1]\n",
    "D = D_user + D_item\n",
    "print(\"Final feature dimension D =\", D)\n",
    "\n",
    "# construir user_full: [user_features , zeros(n_users, D_item)]\n",
    "user_pad = np.zeros((n_users, D_item), dtype=float)\n",
    "user_full = np.hstack([user_features, user_pad])   # shape (n_users, D)\n",
    "\n",
    "# construir item_full: [zeros(n_items_train, D_user) , item_features]\n",
    "item_pad = np.zeros((n_items_train, D_user), dtype=float)\n",
    "item_full = np.hstack([item_pad, item_features])   # shape (n_items_train, D)\n",
    "\n",
    "# agora empilhar em X_all mantendo ordem users then train items\n",
    "X_all = np.vstack([user_full, item_full]).astype(float)   # shape (n_users + n_items_train, D)\n",
    "X_all_tensor = torch.tensor(np.nan_to_num(X_all), dtype=torch.float)\n",
    "\n",
    "# salvar\n",
    "torch.save(X_all_tensor, \"X_all_train.pt\")\n",
    "np.save(\"user_features_train_full.npy\", user_full)\n",
    "np.save(\"item_features_train_full.npy\", item_full)\n",
    "with open(\"dims.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"n_users\": n_users, \"n_train_items\": n_items_train, \"feature_dim\": D, \"D_user\": D_user, \"D_item\": D_item}, f)\n",
    "\n",
    "print(\"Saved X_all_train.pt  shape:\", X_all_tensor.shape)\n",
    "print(\"user_full shape:\", user_full.shape)\n",
    "print(\"item_full shape:\", item_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c17379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar itens de teste em vetores compatíveis com X_all\n",
    "\n",
    "# carregar encoders e dims previamente salvos\n",
    "ohe_release = joblib.load(\"ohe_release.joblib\")\n",
    "mlb_genres = joblib.load(\"mlb_genres.joblib\")\n",
    "\n",
    "with open(\"dims.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dims = json.load(f)\n",
    "D_user = int(dims[\"D_user\"])\n",
    "D_item = int(dims[\"D_item\"])\n",
    "D = int(dims[\"feature_dim\"])\n",
    "\n",
    "# utilitário para parse de lista [\"k:v\", ...] -> dict\n",
    "def parse_kv_list_to_dict(kv_list):\n",
    "    d = {}\n",
    "    for kv in kv_list:\n",
    "        if isinstance(kv, str) and \":\" in kv:\n",
    "            k,v = kv.split(\":\",1)\n",
    "            d.setdefault(k, []).append(v)\n",
    "    return d\n",
    "\n",
    "# transformar um único item (lista de tokens \"k:v\") em vetor item-part (D_item,)\n",
    "def transform_single_test_item_itempart(kv_list):\n",
    "    parsed = parse_kv_list_to_dict(kv_list)\n",
    "    # releaseDate token (string) -> shape (1, )\n",
    "    rd = parsed.get('releaseDate', [\"\"])[0] if parsed.get('releaseDate') else \"\"\n",
    "    if rd:\n",
    "        try:\n",
    "            rd_arr = np.array([rd]).reshape(-1,1)\n",
    "            rd_vec = ohe_release.transform(rd_arr)  # shape (1, D_release)\n",
    "        except Exception:\n",
    "            # fallback: unseen or bad -> zeros\n",
    "            rd_vec = np.zeros((1, ohe_release.transform(np.array([ohe_release.categories_[0][0]]).reshape(-1,1)).shape[1]))\n",
    "    else:\n",
    "        rd_vec = np.zeros((1, ohe_release.transform(np.array([ohe_release.categories_[0][0]]).reshape(-1,1)).shape[1]))\n",
    "    # genres list -> shape (1, n_genres)\n",
    "    genres = parsed.get('genre', [])\n",
    "    if isinstance(genres, str):\n",
    "        genres = [genres]\n",
    "    try:\n",
    "        genres_vec = mlb_genres.transform([genres])  # (1, n_genres)\n",
    "    except Exception:\n",
    "        genres_vec = np.zeros((1, len(mlb_genres.classes_)))\n",
    "    # concatenar partes do item (ordenadas exatamente como item_features)\n",
    "    item_part = np.hstack([rd_vec, genres_vec]).astype(float).ravel()\n",
    "    # garantir dimensão D_item (pad ou trim se necessário)\n",
    "    if item_part.shape[0] < D_item:\n",
    "        pad = np.zeros((D_item - item_part.shape[0],), dtype=float)\n",
    "        item_part = np.hstack([item_part, pad])\n",
    "    elif item_part.shape[0] > D_item:\n",
    "        item_part = item_part[:D_item]\n",
    "    return item_part\n",
    "\n",
    "# transformar um lote de items (lista de kv_lists) e retornar matriz full (n_items, D)\n",
    "def transform_test_items_batch(kv_lists, save_path=None):\n",
    "    item_parts = [transform_single_test_item_itempart(kv) for kv in kv_lists]\n",
    "    item_parts = np.vstack(item_parts)  # shape (n_items, D_item)\n",
    "    # construir full vectors: [zeros(D_user) , item_part]\n",
    "    user_zeros = np.zeros((item_parts.shape[0], D_user), dtype=float)\n",
    "    full = np.hstack([user_zeros, item_parts])  # shape (n_items, D)\n",
    "    # sanity: ensure correct width\n",
    "    if full.shape[1] != D:\n",
    "        # try to pad or trim to exact D\n",
    "        if full.shape[1] < D:\n",
    "            pad = np.zeros((full.shape[0], D - full.shape[1]), dtype=float)\n",
    "            full = np.hstack([full, pad])\n",
    "        else:\n",
    "            full = full[:, :D]\n",
    "    if save_path:\n",
    "        np.save(save_path, full)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num positive pairs: 95961\n",
      "Epoch 01/30  loss=0.5659\n",
      "Epoch 02/30  loss=0.4773\n",
      "Epoch 03/30  loss=0.4544\n",
      "Epoch 04/30  loss=0.4440\n",
      "Epoch 05/30  loss=0.4421\n",
      "Epoch 06/30  loss=0.4398\n",
      "Epoch 07/30  loss=0.4378\n",
      "Epoch 08/30  loss=0.4381\n",
      "Epoch 09/30  loss=0.4372\n",
      "Epoch 10/30  loss=0.4361\n",
      "Epoch 11/30  loss=0.4361\n",
      "Epoch 12/30  loss=0.4342\n",
      "Epoch 13/30  loss=0.4340\n",
      "Epoch 14/30  loss=0.4346\n",
      "Epoch 15/30  loss=0.4336\n",
      "Epoch 16/30  loss=0.4339\n",
      "Epoch 17/30  loss=0.4338\n",
      "Epoch 18/30  loss=0.4334\n",
      "Epoch 19/30  loss=0.4335\n",
      "Epoch 20/30  loss=0.4337\n",
      "Epoch 21/30  loss=0.4330\n",
      "Epoch 22/30  loss=0.4342\n",
      "Epoch 23/30  loss=0.4327\n",
      "Epoch 24/30  loss=0.4327\n",
      "Epoch 25/30  loss=0.4322\n",
      "Epoch 26/30  loss=0.4322\n",
      "Epoch 27/30  loss=0.4337\n",
      "Epoch 28/30  loss=0.4344\n",
      "Epoch 29/30  loss=0.4329\n",
      "Epoch 30/30  loss=0.4336\n",
      "Saved encoder and embeddings. user_embeddings shape: (943, 32) item_embeddings_train shape: (1652, 32)\n"
     ]
    }
   ],
   "source": [
    "# Treino GraphSAGE model\n",
    "# Carregar artefatos\n",
    "edge_index = torch.load(\"edge_index_train.pt\")                # shape [2, E]\n",
    "X_all = torch.load(\"X_all_train.pt\")                          # shape [n_nodes, D]\n",
    "with open(\"dims.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    dims = json.load(f)\n",
    "n_users = int(dims[\"n_users\"])\n",
    "n_train_items = int(dims[\"n_train_items\"])\n",
    "D = int(dims[\"feature_dim\"])\n",
    "\n",
    "# Dispositivos\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "\n",
    "# Preparar pares positivos (u_idx, i_idx_global)\n",
    "#Reconstruir a lista pos_pairs a partir de edge_index: filtrar arestas user->item\n",
    "ei = edge_index.cpu().numpy()\n",
    "src = ei[0]; dst = ei[1]\n",
    "# edges user->item têm src < n_users e dst >= n_users\n",
    "mask = (src < n_users) & (dst >= n_users)\n",
    "pos_u = src[mask]\n",
    "pos_i = dst[mask]\n",
    "pos_pairs = list(zip(pos_u.tolist(), pos_i.tolist()))  # i já é global (n_users + local_item_idx)\n",
    "\n",
    "print(\"num positive pairs:\", len(pos_pairs))\n",
    "\n",
    "# negative sampling helper: sample random item global index (n_users .. n_users+n_train_items-1)\n",
    "all_item_globals = np.arange(n_users, n_users + n_train_items)\n",
    "\n",
    "def sample_negatives(batch_users, k=1):\n",
    "    # retorna array shape (B, k) de itens globais\n",
    "    negs = []\n",
    "    for u in batch_users:\n",
    "        choices = np.random.choice(all_item_globals, size=k, replace=True)\n",
    "        negs.append(choices)\n",
    "    return np.array(negs)\n",
    "\n",
    "# GraphSAGE encoder\n",
    "class GraphSAGEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128, out_dim=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        if num_layers == 1:\n",
    "            self.convs.append(SAGEConv(in_dim, out_dim))\n",
    "        else:\n",
    "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
    "            for _ in range(num_layers-2):\n",
    "                self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.convs.append(SAGEConv(hidden_dim, out_dim))\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "in_dim = X_all.shape[1]\n",
    "hidden_dim = 64\n",
    "out_dim = 32\n",
    "num_layers = 1\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-6\n",
    "epochs = 30\n",
    "batch_size = 512  # ajuste conforme seu dataset/ memória; para poucos pares use len(pos_pairs)\n",
    "\n",
    "# instanciar modelo e optimizer\n",
    "encoder = GraphSAGEEncoder(in_dim, hidden_dim, out_dim, num_layers).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# mover tensores para DEVICE\n",
    "X_all = X_all.to(DEVICE)\n",
    "edge_index = edge_index.to(DEVICE)\n",
    "\n",
    "# treino\n",
    "for epoch in range(1, epochs+1):\n",
    "    encoder.train()\n",
    "    random.shuffle(pos_pairs)\n",
    "    total_loss = 0.0\n",
    "    # se dataset pequeno, processa tudo de uma vez\n",
    "    if batch_size >= len(pos_pairs):\n",
    "        batch = pos_pairs\n",
    "        batch_users = [p[0] for p in batch]\n",
    "        batch_pos_items = [p[1] for p in batch]\n",
    "        neg_items = sample_negatives(batch_users, k=1)[:,0].tolist()\n",
    "        z = encoder(X_all, edge_index)  # embeddings para todos os nós\n",
    "        u_emb = z[torch.tensor(batch_users, device=DEVICE)]\n",
    "        pos_emb = z[torch.tensor(batch_pos_items, device=DEVICE)]\n",
    "        neg_emb = z[torch.tensor(neg_items, device=DEVICE)]\n",
    "        pos_scores = (u_emb * pos_emb).sum(dim=-1)\n",
    "        neg_scores = (u_emb * neg_emb).sum(dim=-1)\n",
    "        loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-15).mean()\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss = loss.item()\n",
    "    else:\n",
    "        # mini-batch loop\n",
    "        for i in range(0, len(pos_pairs), batch_size):\n",
    "            batch = pos_pairs[i:i+batch_size]\n",
    "            batch_users = [p[0] for p in batch]\n",
    "            batch_pos_items = [p[1] for p in batch]\n",
    "            neg_items = sample_negatives(batch_users, k=1)[:,0].tolist()\n",
    "            z = encoder(X_all, edge_index)\n",
    "            u_emb = z[torch.tensor(batch_users, device=DEVICE)]\n",
    "            pos_emb = z[torch.tensor(batch_pos_items, device=DEVICE)]\n",
    "            neg_emb = z[torch.tensor(neg_items, device=DEVICE)]\n",
    "            pos_scores = (u_emb * pos_emb).sum(dim=-1)\n",
    "            neg_scores = (u_emb * neg_emb).sum(dim=-1)\n",
    "            loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-15).mean()\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            total_loss += loss.item() * len(batch)\n",
    "        total_loss = total_loss / len(pos_pairs)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{epochs}  loss={total_loss:.4f}\")\n",
    "\n",
    "# salvar encoder e embeddings\n",
    "torch.save(encoder.state_dict(), \"graphsage_encoder.pt\")\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    z_all = encoder(X_all, edge_index).cpu().numpy()  # shape (n_nodes, out_dim)\n",
    "user_embeddings = z_all[:n_users]\n",
    "item_embeddings_train = z_all[n_users:]  # correspond to train items in item2idx order\n",
    "\n",
    "np.save(\"user_embeddings.npy\", user_embeddings)\n",
    "np.save(\"item_embeddings_train.npy\", item_embeddings_train)\n",
    "print(\"Saved encoder and embeddings. user_embeddings shape:\", user_embeddings.shape, \"item_embeddings_train shape:\", item_embeddings_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dd84d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. test_embeddings shape: (30, 32)\n",
      "Saved mapping test_item_id2idx.json (id -> row index in test_item_embeddings.npy).\n"
     ]
    }
   ],
   "source": [
    "# Inferência de embeddings para itens de teste\n",
    "# inscrever itens de teste (isolados) e obter embeddings com encoder treinado\n",
    "\n",
    "# Carregar artefatos\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_all_train = torch.load(\"X_all_train.pt\")           # (n_nodes_train, D)\n",
    "edge_index = torch.load(\"edge_index_train.pt\")       # (2, E)\n",
    "with open(\"dims.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    dims = json.load(f)\n",
    "n_nodes_train = X_all_train.shape[0]\n",
    "D = int(dims[\"feature_dim\"])\n",
    "\n",
    "# carregar encoder treinado\n",
    "in_dim = X_all_train.shape[1]\n",
    "encoder = GraphSAGEEncoder(in_dim, hidden_dim=64, out_dim=32, num_layers=1).to(DEVICE)\n",
    "encoder.load_state_dict(torch.load(\"graphsage_encoder.pt\", map_location=DEVICE))\n",
    "encoder.eval()\n",
    "\n",
    "# Construir X_test_full: use transform_test_items_batch(kv_lists)\n",
    "# Assuma que `test_kv_lists` está definido e alinhado com `test_item_ids`\n",
    "# Se não estiver, construa test_kv_lists a partir de metadata_items e a lista de test_item_ids presentes\n",
    "try:\n",
    "    test_kv_lists\n",
    "except NameError:\n",
    "    test_kv_lists = [ item_features_raw.get(str(iid), []) for iid in test_item_ids ]\n",
    "\n",
    "# Transformar para matriz full (n_test, D) — função definida na Célula 6\n",
    "X_test_full = transform_test_items_batch(test_kv_lists, save_path=None)  # numpy (n_test, D)\n",
    "assert X_test_full.shape[1] == D, \"Dimensão incompatível: ajuste encoders/padding\"\n",
    "\n",
    "# Concatenar sem alterar edge_index: novos nós estarão isolados (ou conectados se preferir)\n",
    "X_all_extended = np.vstack([ X_all_train.cpu().numpy(), X_test_full ])   # (n_nodes_train + n_test, D)\n",
    "X_all_ext_t = torch.tensor(X_all_extended, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Rodar forward do encoder (edge_index permanece o do grafo de treino)\n",
    "edge_index = edge_index.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    embeddings_all = encoder(X_all_ext_t, edge_index)   # (n_nodes_train + n_test, out_dim)\n",
    "emb_np = embeddings_all.cpu().numpy()\n",
    "\n",
    "# Extrair embeddings dos itens de teste: índices test_global = range(n_nodes_train, n_nodes_train + n_test)\n",
    "n_test = X_test_full.shape[0]\n",
    "test_global_start = n_nodes_train\n",
    "test_global_indices = np.arange(test_global_start, test_global_start + n_test)\n",
    "test_embeddings = emb_np[test_global_indices, :]\n",
    "\n",
    "# salvar embeddings e mapping id->index\n",
    "np.save(\"test_item_embeddings.npy\", test_embeddings)\n",
    "\n",
    "mapping = { str(test_item_ids[i]): int(i) for i in range(len(test_item_ids)) }\n",
    "with open(\"test_item_id2idx.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping,f)\n",
    "\n",
    "print(\"Done. test_embeddings shape:\", test_embeddings.shape)\n",
    "print(\"Saved mapping test_item_id2idx.json (id -> row index in test_item_embeddings.npy).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "315e4678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test_items_total: 30\n",
      "n_test_items_with_positives: 30\n"
     ]
    }
   ],
   "source": [
    "# Carregar artefatos e construir ground truth sets por item\n",
    "\n",
    "# Carregar embeddings e mapas\n",
    "user_embeddings = np.load(\"user_embeddings.npy\")            # shape (n_users, emb_dim)\n",
    "item_embeddings_test = np.load(\"test_item_embeddings.npy\")  # shape (n_test, emb_dim)\n",
    "with open(\"test_item_id2idx.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    test_id2row = json.load(f)     # map item_id -> row index in item_embeddings_test\n",
    "\n",
    "# Normalizar tipos\n",
    "test_interactions['user_id'] = test_interactions['user_id'].astype(str)\n",
    "test_interactions['item_id'] = test_interactions['item_id'].astype(str)\n",
    "\n",
    "# Construir ground truth: dict item_id -> set(user_idx)\n",
    "gt_item2users = {}\n",
    "for _, row in test_interactions.iterrows():\n",
    "    iid = str(row['item_id'])\n",
    "    uid = str(row['user_id'])\n",
    "    if iid in test_id2row and uid in user2idx:\n",
    "        r = test_id2row[iid]\n",
    "        uidx = int(user2idx[uid])\n",
    "        gt_item2users.setdefault(iid, set()).add(uidx)\n",
    "\n",
    "# Reduzir para items de teste que têm pelo menos um positivo (avaliáveis)\n",
    "evaluated_items = [iid for iid, s in gt_item2users.items() if len(s) > 0]\n",
    "print(\"n_test_items_total:\", len(test_id2row))\n",
    "print(\"n_test_items_with_positives:\", len(evaluated_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a59206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed top-50 users for each test item (by row index).\n"
     ]
    }
   ],
   "source": [
    "# Normalizar embeddings para usar dot = cosine\n",
    "def l2_normalize_rows(X):\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return X / norms\n",
    "\n",
    "U = l2_normalize_rows(user_embeddings)         # (n_users, d)\n",
    "I = l2_normalize_rows(item_embeddings_test)     # (n_test, d)\n",
    "\n",
    "# produto matriz I @ U.T -> (n_test, n_users) de similaridades\n",
    "sims = I.dot(U.T)  # cuidado com memória se n_test * n_users grande; seu dataset ML-100k razoável\n",
    "\n",
    "K_max = 50\n",
    "# para cada test item (row index), recuperar top-K_max user indices (descendente)\n",
    "topk_users_per_test = {}\n",
    "n_test = I.shape[0]\n",
    "for test_i in range(n_test):\n",
    "    row = sims[test_i]\n",
    "    # argsort decrescente; usar partition para eficiência\n",
    "    if row.shape[0] <= K_max:\n",
    "        topk = np.argsort(-row)\n",
    "    else:\n",
    "        idx_part = np.argpartition(-row, K_max-1)[:K_max]\n",
    "        topk = idx_part[np.argsort(-row[idx_part])]\n",
    "    topk_users_per_test[test_i] = topk  # array of user indices, length <= K_max\n",
    "\n",
    "print(\"Computed top-50 users for each test item (by row index).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9842e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_evaluated_items: 30\n",
      "Precision@10: 0.3100    NDCG@10: 0.3349\n",
      "Precision@20: 0.2683    NDCG@20: 0.2959\n",
      "Precision@50: 0.2487    NDCG@50: 0.2679\n"
     ]
    }
   ],
   "source": [
    "# Calcular Precision@K e NDCG@K médios, para K=10, 20 e 50, nos nós de teste\n",
    "\n",
    "def precision_at_k(recommended, ground_truth_set, k):\n",
    "    rec_k = recommended[:k]\n",
    "    hits = sum(1 for u in rec_k if u in ground_truth_set)\n",
    "    return hits / k\n",
    "\n",
    "def dcg_at_k(recommended, ground_truth_set, k):\n",
    "    rec_k = recommended[:k]\n",
    "    dcg = 0.0\n",
    "    for i, u in enumerate(rec_k):\n",
    "        rel = 1.0 if u in ground_truth_set else 0.0\n",
    "        denom = log2(i+2)  # i starts at 0 -> position 1 -> log2(2)\n",
    "        dcg += rel / denom\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k(ground_truth_set, k):\n",
    "    # ideal DCG has min(len(gt), k) ones at top\n",
    "    n_rel = min(len(ground_truth_set), k)\n",
    "    idcg = sum(1.0 / log2(i+2) for i in range(n_rel))\n",
    "    return idcg if idcg > 0 else 1.0  # avoid div0; if no positives, we will skip such item\n",
    "\n",
    "Ks = [10, 20, 50]\n",
    "sum_prec = {k: 0.0 for k in Ks}\n",
    "sum_ndcg = {k: 0.0 for k in Ks}\n",
    "count_items = 0\n",
    "\n",
    "for iid in evaluated_items:\n",
    "    row_idx = test_id2row[iid]\n",
    "    recommended = topk_users_per_test[row_idx].tolist()\n",
    "    gt_set = gt_item2users[iid]\n",
    "    if len(gt_set) == 0:\n",
    "        continue\n",
    "    count_items += 1\n",
    "    for k in Ks:\n",
    "        prec = precision_at_k(recommended, gt_set, k)\n",
    "        sum_prec[k] += prec\n",
    "        dcg = dcg_at_k(recommended, gt_set, k)\n",
    "        idcg = idcg_at_k(gt_set, k)\n",
    "        ndcg = dcg / idcg\n",
    "        sum_ndcg[k] += ndcg\n",
    "\n",
    "# Médias\n",
    "avg_prec = {k: (sum_prec[k] / count_items) for k in Ks}\n",
    "avg_ndcg = {k: (sum_ndcg[k] / count_items) for k in Ks}\n",
    "\n",
    "print(\"n_evaluated_items:\", count_items)\n",
    "for k in Ks:\n",
    "    print(f\"Precision@{k}: {avg_prec[k]:.4f}    NDCG@{k}: {avg_ndcg[k]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87127bf",
   "metadata": {},
   "source": [
    "Configuração de hiperparâmetros:\n",
    "in_dim = X_all.shape[1]\n",
    "hidden_dim = 128\n",
    "out_dim = 64\n",
    "num_layers = 2\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "epochs = 30\n",
    "batch_size = 1024\n",
    "\n",
    "Resultados:\n",
    "n_evaluated_items: 30\n",
    "Precision@10: 0.1833    NDCG@10: 0.1910\n",
    "Precision@20: 0.2000    NDCG@20: 0.1997\n",
    "Precision@50: 0.2153    NDCG@50: 0.2120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f81b0",
   "metadata": {},
   "source": [
    "Configuração de hiperparâmetros:\n",
    "in_dim = X_all.shape[1]\n",
    "hidden_dim = 128\n",
    "out_dim = 64\n",
    "num_layers = 2\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "epochs = 30\n",
    "batch_size = 1024\n",
    "\n",
    "Resultados:\n",
    "n_evaluated_items: 30\n",
    "Precision@10: 0.2067    NDCG@10: 0.2160\n",
    "Precision@20: 0.1767    NDCG@20: 0.1913\n",
    "Precision@50: 0.1620    NDCG@50: 0.1734"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
