{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\José Walter\\Mestrado\\3. TrabalhoFinal\\GraphEmbedding\\app\\kgenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import random\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "from datetime import timezone\n",
    "# Default path to data files\n",
    "PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5451fa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\José Walter Lima\\AppData\\Local\\Temp\\ipykernel_3412\\3441961429.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  interaction_data = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000209, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1     1193       1\n",
       "1        1      661       1\n",
       "2        1      914       1\n",
       "3        1     3408       1\n",
       "4        1     2355       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load user-item interaction data\n",
    "interaction_data = pd.read_csv(\n",
    "    PATH + 'ml-1m/ratings.dat',\n",
    "    sep='::',\n",
    "    names=['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    )[['user_id', 'item_id', 'rating']]\n",
    "interaction_data['rating'] = 1\n",
    "display(interaction_data.shape)\n",
    "interaction_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ada4bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items on test set: 297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3408', '2687', '3186', '2762', '3114']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load test item IDs from the json file saved\n",
    "# previously from Knowledge Graph Method\n",
    "with open('../experiments/test_ids.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# Extract test item IDs as integers\n",
    "test_item_ids = [item['movieId'] for item in data]\n",
    "print(f\"items on test set: {len(test_item_ids)}\")\n",
    "display(test_item_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e2d6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split interaction data into train and test sets\n",
    "# to allow later cold start evaluation \n",
    "train_interactions = interaction_data[\n",
    "    ~interaction_data['item_id'].astype(str).isin(test_item_ids)].reset_index(drop=True)\n",
    "test_interactions = interaction_data[\n",
    "    interaction_data['item_id'].astype(str).isin(test_item_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cff6db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=6040, n_train_items=3409\n"
     ]
    }
   ],
   "source": [
    "# garantir que ids sejam strings para comparação segura\n",
    "train_interactions['user_id'] = train_interactions['user_id'].astype(str)\n",
    "train_interactions['item_id'] = train_interactions['item_id'].astype(str)\n",
    "test_item_ids = [str(x) for x in test_item_ids]  # já definido antes\n",
    "\n",
    "# criar lista ordenada de users observados no treino e items que permaneceram no grafo de treino\n",
    "unique_user_ids = sorted(train_interactions['user_id'].unique().tolist())\n",
    "train_item_ids = sorted(train_interactions['item_id'].unique().tolist())  # exclui itens de teste\n",
    "\n",
    "# construir mapas id -> índice\n",
    "user2idx = {u: i for i, u in enumerate(unique_user_ids)}\n",
    "item2idx = {v: i for i, v in enumerate(train_item_ids)}\n",
    "\n",
    "# contar e salvar (opcional, mas útil)\n",
    "n_users = len(user2idx)\n",
    "n_train_items = len(item2idx)\n",
    "print(f\"n_users={n_users}, n_train_items={n_train_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4933c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\José Walter Lima\\AppData\\Local\\Temp\\ipykernel_3412\\74960458.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  edge_index_train = torch.tensor([src, dst], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_train shape: torch.Size([2, 1749866])\n",
      "num_edges (directed count): 1749866\n",
      "num_unique_nodes referenced: 9449\n"
     ]
    }
   ],
   "source": [
    "# Mapear colunas para índices locais\n",
    "train_df = train_interactions.copy()\n",
    "train_df['u_idx'] = train_df['user_id'].map(user2idx)\n",
    "train_df['i_idx_local'] = train_df['item_id'].map(item2idx)  # items de treino apenas\n",
    "\n",
    "# Remover linhas sem mapeamento (segurança)\n",
    "train_df = train_df.dropna(subset=['u_idx','i_idx_local']).astype({'u_idx':int, 'i_idx_local':int})\n",
    "\n",
    "# Construir índices globais: shift nos items para evitar sobreposição com users\n",
    "u_nodes = train_df['u_idx'].values                      # já 0..n_users-1\n",
    "i_nodes = train_df['i_idx_local'].values + n_users     # items -> n_users .. n_users + n_train_items - 1\n",
    "\n",
    "# Duplicar para grafo não direcionado: user->item e item->user\n",
    "src = np.concatenate([u_nodes, i_nodes])\n",
    "dst = np.concatenate([i_nodes, u_nodes])\n",
    "\n",
    "edge_index_train = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "# Sanity checks e salvar\n",
    "print(\"edge_index_train shape:\", edge_index_train.shape)\n",
    "print(\"num_edges (directed count):\", edge_index_train.shape[1])\n",
    "print(\"num_unique_nodes referenced:\", int(torch.unique(edge_index_train).numel()))\n",
    "# salvar para uso posterior\n",
    "torch.save(edge_index_train, \"edge_index_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4212c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', ['age:1', 'gender:F', 'occupation:10', 'zipcode:48'])\n",
      "('2', ['age:56', 'gender:M', 'occupation:16', 'zipcode:70'])\n",
      "('3', ['age:25', 'gender:M', 'occupation:15', 'zipcode:55'])\n",
      "('4', ['age:45', 'gender:M', 'occupation:7', 'zipcode:02'])\n",
      "('5', ['age:25', 'gender:M', 'occupation:20', 'zipcode:55'])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a defaultdict to hold user features\n",
    "user_data = defaultdict(dict)\n",
    "\n",
    "# Read data and build user features dictionary\n",
    "def load_feature(file_path, feature_name):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            userId = row['userId']\n",
    "            value = row[feature_name]\n",
    "            user_data[userId][feature_name] = value\n",
    "\n",
    "# Load each feature file\n",
    "load_feature(PATH + 'ageRel.csv', 'age')\n",
    "load_feature(PATH + 'genderRel.csv', 'gender')\n",
    "load_feature(PATH + 'occupationRel.csv', 'occupation')\n",
    "load_feature(PATH + 'residesRel.csv', 'zipcode')\n",
    "\n",
    "# Build user features as a dictionary (user_id -> list of \"k:v\" features)\n",
    "user_features_raw = {\n",
    "    str(userId): [\n",
    "        f'age:{data.get(\"age\",\"\")}',\n",
    "        f'gender:{data.get(\"gender\",\"\")}',\n",
    "        f'occupation:{data.get(\"occupation\",\"\")}',\n",
    "        f'zipcode:{data.get(\"zipcode\",\"\")}'\n",
    "    ]\n",
    "    for userId, data in user_data.items()\n",
    "}\n",
    "\n",
    "# Display first 5 user features\n",
    "for item in list(user_features_raw.items())[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba714d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', ['releaseDate:1995', 'genre:Animation', \"genre:Children's\", 'genre:Comedy'])\n",
      "('2', ['releaseDate:1995', 'genre:Adventure', \"genre:Children's\", 'genre:Fantasy'])\n",
      "('3', ['releaseDate:1995', 'genre:Comedy', 'genre:Romance'])\n",
      "('4', ['releaseDate:1995', 'genre:Comedy', 'genre:Drama'])\n",
      "('5', ['releaseDate:1995', 'genre:Comedy'])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a defaultdict to hold item features\n",
    "item_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Read data and build item features dictionary\n",
    "# Modified version to handle multiple genres\n",
    "def load_feature(file_path, feature_name):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            itemId = row['movieId']\n",
    "            value = row[feature_name]\n",
    "            if feature_name == 'genreDesc':\n",
    "                item_data[itemId]['genre'].append(value)\n",
    "            else:\n",
    "                item_data[itemId][feature_name] = value\n",
    "\n",
    "# Load each feature file\n",
    "load_feature(PATH + 'releaseRel.csv', 'releaseDate')\n",
    "load_feature(PATH + 'genreRel.csv', 'genreDesc')\n",
    "\n",
    "# Build item features as a dictionary (item_id -> list of \"k:v\" features)\n",
    "item_features_raw = {\n",
    "    str(itemId): (\n",
    "        [f'releaseDate:{data.get(\"releaseDate\",\"\")}'] +\n",
    "        [f'genre:{genre}' for genre in data.get('genre', [])]\n",
    "    )\n",
    "    for itemId, data in item_data.items()\n",
    "}\n",
    "\n",
    "# Display first 5 item features\n",
    "for item in list(item_features_raw.items())[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb2b5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper para parse \"k:v\" lists\n",
    "def parse_kv_list(kv_list):\n",
    "    d = defaultdict(list)\n",
    "    for kv in kv_list:\n",
    "        if isinstance(kv, str) and \":\" in kv:\n",
    "            k,v = kv.split(\":\",1)\n",
    "            d[k].append(v)\n",
    "    # return simplified: single-value -> string, multi -> list\n",
    "    out = {}\n",
    "    for k,vals in d.items():\n",
    "        out[k] = vals[0] if len(vals)==1 else vals\n",
    "    return out\n",
    "\n",
    "# garantir chaves strings\n",
    "user_keys = list(user2idx.keys())        # ordem já definida antes\n",
    "train_item_keys = list(item2idx.keys())  # itens que estão no grafo de treino\n",
    "\n",
    "# criar parsed lists (preserve order)\n",
    "users_parsed = []\n",
    "for uid in user_keys:\n",
    "    kvs = user_features_raw.get(str(uid), [])   # ajuste: metadata map may use string keys\n",
    "    parsed = parse_kv_list(kvs)\n",
    "    parsed['id'] = uid\n",
    "    users_parsed.append(parsed)\n",
    "df_users = pd.DataFrame(users_parsed).set_index('id').fillna(\"\")\n",
    "\n",
    "items_parsed = []\n",
    "for iid in train_item_keys:\n",
    "    kvs = item_features_raw.get(str(iid), [])\n",
    "    parsed = parse_kv_list(kvs)\n",
    "    parsed['id'] = iid\n",
    "    items_parsed.append(parsed)\n",
    "df_items = pd.DataFrame(items_parsed).set_index('id').fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409b6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users: 6040\n",
      "user_age shape: (6040, 1)\n",
      "user_gender sample: ['F' 'F' 'M']\n",
      "user_occupation sample: ['10' '1' '17']\n",
      "user_zipcode sample: ['48' '95' '95']\n",
      "n_train_items: 3409\n",
      "item_release sample: ['1995', '1995', '1996']\n",
      "item_genres sample: [['Animation', \"Children's\", 'Comedy'], ['Action', 'Adventure', 'Thriller'], ['Drama', 'Thriller']]\n"
     ]
    }
   ],
   "source": [
    "# USERS\n",
    "# age numeric (fallback 0), gender string, occupation string, zipcode string\n",
    "user_age = df_users['age'].astype(float).values.reshape(-1,1) if 'age' in df_users.columns else np.zeros((len(df_users),1))\n",
    "user_gender = df_users['gender'].astype(str).values.reshape(-1,1) if 'gender' in df_users.columns else np.array([[\"\"]] * len(df_users))\n",
    "user_occupation = df_users['occupation'].astype(str).values.reshape(-1,1) if 'occupation' in df_users.columns else np.array([[\"\"]] * len(df_users))\n",
    "user_zipcode = df_users['zipcode'].astype(str).values.reshape(-1,1) if 'zipcode' in df_users.columns else np.array([[\"\"]] * len(df_users))\n",
    "\n",
    "# ITEMS\n",
    "# releaseDate kept as original string (e.g., \"Jan-1995\")\n",
    "# genres: ensure list for each item\n",
    "item_release_str = []\n",
    "item_genres_list = []\n",
    "for iid in df_items.index:\n",
    "    rd = df_items.loc[iid].get('releaseDate', \"\")\n",
    "    # keep original string (or empty string if missing)\n",
    "    item_release_str.append(rd if isinstance(rd, str) else \"\")\n",
    "    # genres: parsed could be list or single string\n",
    "    genres = df_items.loc[iid].get('genre', [])\n",
    "    if isinstance(genres, str):\n",
    "        genres = [genres] if genres else []\n",
    "    item_genres_list.append(genres)\n",
    "\n",
    "# Converter para arrays numpy apropriados\n",
    "# user_age already shaped (n_users,1)\n",
    "# user_gender, user_occupation, user_zipcode shaped (n_users,1)\n",
    "user_cat_inputs = {\n",
    "    'gender': user_gender,\n",
    "    'occupation': user_occupation,\n",
    "    'zipcode': user_zipcode\n",
    "}\n",
    "\n",
    "# item_release_str is a list of strings length n_items\n",
    "# item_genres_list is a list of lists length n_items\n",
    "\n",
    "# Mostre formas e amostras para verificação rápida\n",
    "print(\"n_users:\", len(df_users))\n",
    "print(\"user_age shape:\", user_age.shape)\n",
    "print(\"user_gender sample:\", user_gender[:3].ravel())\n",
    "print(\"user_occupation sample:\", user_occupation[:3].ravel())\n",
    "print(\"user_zipcode sample:\", user_zipcode[:3].ravel())\n",
    "\n",
    "print(\"n_train_items:\", len(df_items))\n",
    "print(\"item_release sample:\", item_release_str[:3])\n",
    "print(\"item_genres sample:\", item_genres_list[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0d63f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_cat_ohe shape: (6040, 123)\n",
      "user_age_scaled shape: (6040, 1)\n",
      "item_release_ohe shape: (3409, 81)\n",
      "genres_mx shape: (3409, 18)\n"
     ]
    }
   ],
   "source": [
    "# USERS\n",
    "# ohe_user: gender, occupation, zipcode combinados\n",
    "ohe_user = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "user_cat_input = np.hstack([user_cat_inputs['gender'], user_cat_inputs['occupation'], user_cat_inputs['zipcode']])  # shape (n_users, 3)\n",
    "user_cat_ohe = ohe_user.fit_transform(user_cat_input)  # (n_users, D_user_cat)\n",
    "\n",
    "# age scaler\n",
    "sc_user_age = StandardScaler().fit(user_age)  # user_age shape (n_users,1)\n",
    "user_age_scaled = sc_user_age.transform(user_age)  # (n_users,1)\n",
    "\n",
    "# ITEMS\n",
    "# releaseDate as categorical token -> OneHotEncoder\n",
    "ohe_release = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "item_release_arr = np.array(item_release_str).reshape(-1,1)  # (n_items,1)\n",
    "item_release_ohe = ohe_release.fit_transform(item_release_arr)  # (n_items, D_release)\n",
    "\n",
    "# genres multi-hot\n",
    "mlb_genres = MultiLabelBinarizer(sparse_output=False)\n",
    "genres_mx = mlb_genres.fit_transform(item_genres_list)  # (n_items, n_genres)\n",
    "\n",
    "# salvar encoders\n",
    "joblib.dump(ohe_user, \"ohe_user.joblib\")\n",
    "joblib.dump(sc_user_age, \"sc_user_age.joblib\")\n",
    "joblib.dump(ohe_release, \"ohe_release.joblib\")\n",
    "joblib.dump(mlb_genres, \"mlb_genres.joblib\")\n",
    "\n",
    "# prints para verificação\n",
    "print(\"user_cat_ohe shape:\", user_cat_ohe.shape)\n",
    "print(\"user_age_scaled shape:\", user_age_scaled.shape)\n",
    "print(\"item_release_ohe shape:\", item_release_ohe.shape)\n",
    "print(\"genres_mx shape:\", genres_mx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eaa2275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user sub-dim: 124\n",
      "item sub-dim: 99\n",
      "Final feature dimension D = 223\n",
      "Saved X_all_train.pt  shape: torch.Size([9449, 223])\n",
      "user_full shape: (6040, 223)\n",
      "item_full shape: (3409, 223)\n"
     ]
    }
   ],
   "source": [
    "# Construir matrizes finais e concatenar (users then train items), salvar artefatos\n",
    "# verificar shapes\n",
    "user_feat_dim = user_age_scaled.shape[1] + user_cat_ohe.shape[1]   # ou user_features.shape[1] se já tiver\n",
    "item_feat_dim = item_release_ohe.shape[1] + genres_mx.shape[1]    # ou item_features.shape[1]\n",
    "\n",
    "print(\"user sub-dim:\", user_feat_dim)\n",
    "print(\"item sub-dim:\", item_feat_dim)\n",
    "\n",
    "# construir matrizes finais separadas (recalcular para evitar confusão)\n",
    "user_features = np.hstack([user_age_scaled, user_cat_ohe]).astype(float)    # shape (n_users, D_u)\n",
    "item_features = np.hstack([item_release_ohe, genres_mx]).astype(float)     # shape (n_items_train, D_i)\n",
    "\n",
    "n_users = user_features.shape[0]\n",
    "n_items_train = item_features.shape[0]\n",
    "\n",
    "# Dimensão comum: concatenação das sub-dimensões (users first, items second)\n",
    "D_user = user_features.shape[1]\n",
    "D_item = item_features.shape[1]\n",
    "D = D_user + D_item\n",
    "print(\"Final feature dimension D =\", D)\n",
    "\n",
    "# construir user_full: [user_features , zeros(n_users, D_item)]\n",
    "user_pad = np.zeros((n_users, D_item), dtype=float)\n",
    "user_full = np.hstack([user_features, user_pad])   # shape (n_users, D)\n",
    "\n",
    "# construir item_full: [zeros(n_items_train, D_user) , item_features]\n",
    "item_pad = np.zeros((n_items_train, D_user), dtype=float)\n",
    "item_full = np.hstack([item_pad, item_features])   # shape (n_items_train, D)\n",
    "\n",
    "# agora empilhar em X_all mantendo ordem users then train items\n",
    "X_all = np.vstack([user_full, item_full]).astype(float)   # shape (n_users + n_items_train, D)\n",
    "X_all_tensor = torch.tensor(np.nan_to_num(X_all), dtype=torch.float)\n",
    "\n",
    "# salvar\n",
    "torch.save(X_all_tensor, \"X_all_train.pt\")\n",
    "np.save(\"user_features_train_full.npy\", user_full)\n",
    "np.save(\"item_features_train_full.npy\", item_full)\n",
    "with open(\"dims.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"n_users\": n_users, \"n_train_items\": n_items_train, \"feature_dim\": D, \"D_user\": D_user, \"D_item\": D_item}, f)\n",
    "\n",
    "print(\"Saved X_all_train.pt  shape:\", X_all_tensor.shape)\n",
    "print(\"user_full shape:\", user_full.shape)\n",
    "print(\"item_full shape:\", item_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c17379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar itens de teste em vetores compatíveis com X_all\n",
    "\n",
    "# carregar encoders e dims previamente salvos\n",
    "ohe_release = joblib.load(\"ohe_release.joblib\")\n",
    "mlb_genres = joblib.load(\"mlb_genres.joblib\")\n",
    "\n",
    "with open(\"dims.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dims = json.load(f)\n",
    "D_user = int(dims[\"D_user\"])\n",
    "D_item = int(dims[\"D_item\"])\n",
    "D = int(dims[\"feature_dim\"])\n",
    "\n",
    "# utilitário para parse de lista [\"k:v\", ...] -> dict\n",
    "def parse_kv_list_to_dict(kv_list):\n",
    "    d = {}\n",
    "    for kv in kv_list:\n",
    "        if isinstance(kv, str) and \":\" in kv:\n",
    "            k,v = kv.split(\":\",1)\n",
    "            d.setdefault(k, []).append(v)\n",
    "    return d\n",
    "\n",
    "# transformar um único item (lista de tokens \"k:v\") em vetor item-part (D_item,)\n",
    "def transform_single_test_item_itempart(kv_list):\n",
    "    parsed = parse_kv_list_to_dict(kv_list)\n",
    "    # releaseDate token (string) -> shape (1, )\n",
    "    rd = parsed.get('releaseDate', [\"\"])[0] if parsed.get('releaseDate') else \"\"\n",
    "    if rd:\n",
    "        try:\n",
    "            rd_arr = np.array([rd]).reshape(-1,1)\n",
    "            rd_vec = ohe_release.transform(rd_arr)  # shape (1, D_release)\n",
    "        except Exception:\n",
    "            # fallback: unseen or bad -> zeros\n",
    "            rd_vec = np.zeros((1, ohe_release.transform(np.array([ohe_release.categories_[0][0]]).reshape(-1,1)).shape[1]))\n",
    "    else:\n",
    "        rd_vec = np.zeros((1, ohe_release.transform(np.array([ohe_release.categories_[0][0]]).reshape(-1,1)).shape[1]))\n",
    "    # genres list -> shape (1, n_genres)\n",
    "    genres = parsed.get('genre', [])\n",
    "    if isinstance(genres, str):\n",
    "        genres = [genres]\n",
    "    try:\n",
    "        genres_vec = mlb_genres.transform([genres])  # (1, n_genres)\n",
    "    except Exception:\n",
    "        genres_vec = np.zeros((1, len(mlb_genres.classes_)))\n",
    "    # concatenar partes do item (ordenadas exatamente como item_features)\n",
    "    item_part = np.hstack([rd_vec, genres_vec]).astype(float).ravel()\n",
    "    # garantir dimensão D_item (pad ou trim se necessário)\n",
    "    if item_part.shape[0] < D_item:\n",
    "        pad = np.zeros((D_item - item_part.shape[0],), dtype=float)\n",
    "        item_part = np.hstack([item_part, pad])\n",
    "    elif item_part.shape[0] > D_item:\n",
    "        item_part = item_part[:D_item]\n",
    "    return item_part\n",
    "\n",
    "# transformar um lote de items (lista de kv_lists) e retornar matriz full (n_items, D)\n",
    "def transform_test_items_batch(kv_lists, save_path=None):\n",
    "    item_parts = [transform_single_test_item_itempart(kv) for kv in kv_lists]\n",
    "    item_parts = np.vstack(item_parts)  # shape (n_items, D_item)\n",
    "    # construir full vectors: [zeros(D_user) , item_part]\n",
    "    user_zeros = np.zeros((item_parts.shape[0], D_user), dtype=float)\n",
    "    full = np.hstack([user_zeros, item_parts])  # shape (n_items, D)\n",
    "    # sanity: ensure correct width\n",
    "    if full.shape[1] != D:\n",
    "        # try to pad or trim to exact D\n",
    "        if full.shape[1] < D:\n",
    "            pad = np.zeros((full.shape[0], D - full.shape[1]), dtype=float)\n",
    "            full = np.hstack([full, pad])\n",
    "        else:\n",
    "            full = full[:, :D]\n",
    "    if save_path:\n",
    "        np.save(save_path, full)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8587cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num positive pairs: 874933\n",
      "\n",
      "=== Run 1/5  seed=43 ===\n",
      "Run 1  Epoch 01/30  loss=0.476539\n",
      "Run 1  Epoch 02/30  loss=0.451884\n",
      "Run 1  Epoch 03/30  loss=0.450458\n",
      "Run 1  Epoch 04/30  loss=0.449422\n",
      "Run 1  Epoch 05/30  loss=0.448949\n",
      "Run 1  Epoch 06/30  loss=0.448671\n",
      "Run 1  Epoch 07/30  loss=0.448765\n",
      "Run 1  Epoch 08/30  loss=0.448782\n",
      "Run 1  Epoch 09/30  loss=0.448500\n",
      "Run 1  Epoch 10/30  loss=0.448350\n",
      "Run 1  Epoch 11/30  loss=0.448328\n",
      "Run 1  Epoch 12/30  loss=0.448282\n",
      "Run 1  Epoch 13/30  loss=0.447598\n",
      "Run 1  Epoch 14/30  loss=0.447378\n",
      "Run 1  Epoch 15/30  loss=0.447720\n",
      "Run 1  Epoch 16/30  loss=0.447471\n",
      "Run 1  Epoch 17/30  loss=0.447655\n",
      "Run 1  Epoch 18/30  loss=0.447576\n",
      "Run 1  Epoch 19/30  loss=0.447299\n",
      "Run 1  Epoch 20/30  loss=0.446888\n",
      "Run 1  Epoch 21/30  loss=0.447164\n",
      "Run 1  Epoch 22/30  loss=0.447256\n",
      "Run 1  Epoch 23/30  loss=0.447189\n",
      "Run 1  Epoch 24/30  loss=0.447205\n",
      "Run 1  Epoch 25/30  loss=0.447273\n",
      "Run 1  Epoch 26/30  loss=0.447336\n",
      "Run 1  Epoch 27/30  loss=0.447869\n",
      "Run 1  Epoch 28/30  loss=0.446923\n",
      "Run 1  Epoch 29/30  loss=0.447132\n",
      "Run 1  Epoch 30/30  loss=0.447307\n",
      "Saved encoder and embeddings for run 1:\n",
      " - encoder: runs_graphsage\\run1_20251027T083914Z\\graphsage_encoder_run1.pt\n",
      " - user_emb: runs_graphsage\\run1_20251027T083914Z\\user_embeddings_run1.npy shape: (6040, 32)\n",
      " - item_emb (train): runs_graphsage\\run1_20251027T083914Z\\item_embeddings_train_run1.npy shape: (3409, 32)\n",
      "\n",
      "=== Run 2/5  seed=44 ===\n",
      "Run 2  Epoch 01/30  loss=0.477432\n",
      "Run 2  Epoch 02/30  loss=0.451354\n",
      "Run 2  Epoch 03/30  loss=0.449948\n",
      "Run 2  Epoch 04/30  loss=0.449282\n",
      "Run 2  Epoch 05/30  loss=0.448400\n",
      "Run 2  Epoch 06/30  loss=0.448489\n",
      "Run 2  Epoch 07/30  loss=0.447999\n",
      "Run 2  Epoch 08/30  loss=0.448059\n",
      "Run 2  Epoch 09/30  loss=0.448080\n",
      "Run 2  Epoch 10/30  loss=0.447833\n",
      "Run 2  Epoch 11/30  loss=0.447741\n",
      "Run 2  Epoch 12/30  loss=0.447784\n",
      "Run 2  Epoch 13/30  loss=0.447639\n",
      "Run 2  Epoch 14/30  loss=0.447881\n",
      "Run 2  Epoch 15/30  loss=0.447433\n",
      "Run 2  Epoch 16/30  loss=0.447462\n",
      "Run 2  Epoch 17/30  loss=0.447799\n",
      "Run 2  Epoch 18/30  loss=0.447038\n",
      "Run 2  Epoch 19/30  loss=0.447455\n",
      "Run 2  Epoch 20/30  loss=0.447385\n",
      "Run 2  Epoch 21/30  loss=0.447918\n",
      "Run 2  Epoch 22/30  loss=0.446903\n",
      "Run 2  Epoch 23/30  loss=0.446836\n",
      "Run 2  Epoch 24/30  loss=0.448126\n",
      "Run 2  Epoch 25/30  loss=0.446907\n",
      "Run 2  Epoch 26/30  loss=0.446838\n",
      "Run 2  Epoch 27/30  loss=0.447395\n",
      "Run 2  Epoch 28/30  loss=0.446933\n",
      "Run 2  Epoch 29/30  loss=0.446887\n",
      "Run 2  Epoch 30/30  loss=0.446870\n",
      "Saved encoder and embeddings for run 2:\n",
      " - encoder: runs_graphsage\\run2_20251027T233000Z\\graphsage_encoder_run2.pt\n",
      " - user_emb: runs_graphsage\\run2_20251027T233000Z\\user_embeddings_run2.npy shape: (6040, 32)\n",
      " - item_emb (train): runs_graphsage\\run2_20251027T233000Z\\item_embeddings_train_run2.npy shape: (3409, 32)\n",
      "\n",
      "=== Run 3/5  seed=45 ===\n",
      "Run 3  Epoch 01/30  loss=0.476446\n",
      "Run 3  Epoch 02/30  loss=0.451730\n",
      "Run 3  Epoch 03/30  loss=0.449599\n",
      "Run 3  Epoch 04/30  loss=0.449303\n",
      "Run 3  Epoch 05/30  loss=0.448562\n",
      "Run 3  Epoch 06/30  loss=0.448483\n",
      "Run 3  Epoch 07/30  loss=0.448549\n",
      "Run 3  Epoch 08/30  loss=0.448487\n",
      "Run 3  Epoch 09/30  loss=0.447932\n",
      "Run 3  Epoch 10/30  loss=0.447328\n",
      "Run 3  Epoch 11/30  loss=0.448111\n",
      "Run 3  Epoch 12/30  loss=0.447850\n",
      "Run 3  Epoch 13/30  loss=0.447558\n",
      "Run 3  Epoch 14/30  loss=0.447743\n",
      "Run 3  Epoch 15/30  loss=0.447569\n",
      "Run 3  Epoch 16/30  loss=0.447812\n",
      "Run 3  Epoch 17/30  loss=0.448032\n",
      "Run 3  Epoch 18/30  loss=0.446929\n",
      "Run 3  Epoch 19/30  loss=0.447444\n",
      "Run 3  Epoch 20/30  loss=0.447232\n",
      "Run 3  Epoch 21/30  loss=0.447419\n",
      "Run 3  Epoch 22/30  loss=0.447753\n",
      "Run 3  Epoch 23/30  loss=0.447020\n",
      "Run 3  Epoch 24/30  loss=0.447829\n",
      "Run 3  Epoch 25/30  loss=0.447363\n",
      "Run 3  Epoch 26/30  loss=0.446908\n",
      "Run 3  Epoch 27/30  loss=0.446940\n",
      "Run 3  Epoch 28/30  loss=0.447421\n",
      "Run 3  Epoch 29/30  loss=0.447313\n",
      "Run 3  Epoch 30/30  loss=0.447460\n",
      "Saved encoder and embeddings for run 3:\n",
      " - encoder: runs_graphsage\\run3_20251028T142312Z\\graphsage_encoder_run3.pt\n",
      " - user_emb: runs_graphsage\\run3_20251028T142312Z\\user_embeddings_run3.npy shape: (6040, 32)\n",
      " - item_emb (train): runs_graphsage\\run3_20251028T142312Z\\item_embeddings_train_run3.npy shape: (3409, 32)\n",
      "\n",
      "=== Run 4/5  seed=46 ===\n",
      "Run 4  Epoch 01/30  loss=0.476477\n",
      "Run 4  Epoch 02/30  loss=0.451548\n",
      "Run 4  Epoch 03/30  loss=0.449842\n",
      "Run 4  Epoch 04/30  loss=0.449308\n",
      "Run 4  Epoch 05/30  loss=0.449313\n",
      "Run 4  Epoch 06/30  loss=0.448624\n",
      "Run 4  Epoch 07/30  loss=0.448521\n",
      "Run 4  Epoch 08/30  loss=0.447947\n",
      "Run 4  Epoch 09/30  loss=0.448064\n",
      "Run 4  Epoch 10/30  loss=0.448314\n",
      "Run 4  Epoch 11/30  loss=0.447674\n",
      "Run 4  Epoch 12/30  loss=0.447823\n",
      "Run 4  Epoch 13/30  loss=0.447304\n",
      "Run 4  Epoch 14/30  loss=0.447466\n",
      "Run 4  Epoch 15/30  loss=0.447335\n",
      "Run 4  Epoch 16/30  loss=0.447281\n",
      "Run 4  Epoch 17/30  loss=0.447480\n",
      "Run 4  Epoch 18/30  loss=0.447045\n",
      "Run 4  Epoch 19/30  loss=0.446921\n",
      "Run 4  Epoch 20/30  loss=0.447397\n",
      "Run 4  Epoch 21/30  loss=0.446930\n",
      "Run 4  Epoch 22/30  loss=0.446556\n",
      "Run 4  Epoch 23/30  loss=0.446923\n",
      "Run 4  Epoch 24/30  loss=0.447054\n",
      "Run 4  Epoch 25/30  loss=0.447301\n",
      "Run 4  Epoch 26/30  loss=0.446634\n",
      "Run 4  Epoch 27/30  loss=0.447093\n",
      "Run 4  Epoch 28/30  loss=0.447226\n",
      "Run 4  Epoch 29/30  loss=0.447254\n",
      "Run 4  Epoch 30/30  loss=0.447182\n",
      "Saved encoder and embeddings for run 4:\n",
      " - encoder: runs_graphsage\\run4_20251029T073740Z\\graphsage_encoder_run4.pt\n",
      " - user_emb: runs_graphsage\\run4_20251029T073740Z\\user_embeddings_run4.npy shape: (6040, 32)\n",
      " - item_emb (train): runs_graphsage\\run4_20251029T073740Z\\item_embeddings_train_run4.npy shape: (3409, 32)\n",
      "\n",
      "=== Run 5/5  seed=47 ===\n",
      "Run 5  Epoch 01/30  loss=0.475657\n",
      "Run 5  Epoch 02/30  loss=0.451767\n",
      "Run 5  Epoch 03/30  loss=0.450321\n",
      "Run 5  Epoch 04/30  loss=0.449279\n",
      "Run 5  Epoch 05/30  loss=0.448846\n",
      "Run 5  Epoch 06/30  loss=0.448553\n",
      "Run 5  Epoch 07/30  loss=0.448176\n",
      "Run 5  Epoch 08/30  loss=0.448245\n",
      "Run 5  Epoch 09/30  loss=0.448193\n",
      "Run 5  Epoch 10/30  loss=0.447784\n",
      "Run 5  Epoch 11/30  loss=0.448167\n",
      "Run 5  Epoch 12/30  loss=0.447832\n",
      "Run 5  Epoch 13/30  loss=0.447643\n",
      "Run 5  Epoch 14/30  loss=0.447736\n",
      "Run 5  Epoch 15/30  loss=0.447667\n",
      "Run 5  Epoch 16/30  loss=0.447640\n",
      "Run 5  Epoch 17/30  loss=0.447509\n",
      "Run 5  Epoch 18/30  loss=0.447492\n",
      "Run 5  Epoch 19/30  loss=0.447484\n",
      "Run 5  Epoch 20/30  loss=0.447110\n",
      "Run 5  Epoch 21/30  loss=0.447483\n",
      "Run 5  Epoch 22/30  loss=0.447300\n",
      "Run 5  Epoch 23/30  loss=0.447134\n",
      "Run 5  Epoch 24/30  loss=0.447417\n",
      "Run 5  Epoch 25/30  loss=0.447203\n",
      "Run 5  Epoch 26/30  loss=0.447587\n",
      "Run 5  Epoch 27/30  loss=0.447039\n",
      "Run 5  Epoch 28/30  loss=0.447536\n",
      "Run 5  Epoch 29/30  loss=0.447322\n",
      "Run 5  Epoch 30/30  loss=0.447123\n",
      "Saved encoder and embeddings for run 5:\n",
      " - encoder: runs_graphsage\\run5_20251029T235944Z\\graphsage_encoder_run5.pt\n",
      " - user_emb: runs_graphsage\\run5_20251029T235944Z\\user_embeddings_run5.npy shape: (6040, 32)\n",
      " - item_emb (train): runs_graphsage\\run5_20251029T235944Z\\item_embeddings_train_run5.npy shape: (3409, 32)\n",
      "\n",
      "All runs finished. Aggregate summary saved to runs_graphsage\\all_runs_summary_20251030T210008Z.json\n"
     ]
    }
   ],
   "source": [
    "# Execução de R runs independentes do treino GraphSAGE\n",
    "\n",
    "R = 5\n",
    "OUT_DIR = \"runs_graphsage\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Hiperparâmetros\n",
    "hidden_dim = 64\n",
    "out_dim = 32\n",
    "num_layers = 1\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-6\n",
    "epochs = 30\n",
    "batch_size = 512\n",
    "\n",
    "# Dispositivos\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Função utilitária para fixar seeds por run\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # determinismo (pode impactar performance; comente se desejar velocidade)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "edge_index = torch.load(\"edge_index_train.pt\")                # shape [2, E]\n",
    "X_all = torch.load(\"X_all_train.pt\")                          # shape [n_nodes, D]\n",
    "with open(\"dims.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    dims = json.load(f)\n",
    "n_users = int(dims[\"n_users\"])\n",
    "n_train_items = int(dims[\"n_train_items\"])\n",
    "D = int(dims[\"feature_dim\"])\n",
    "\n",
    "# Reconstruir pos_pairs a partir de edge_index: filtrar arestas user->item\n",
    "ei = edge_index.cpu().numpy()\n",
    "src = ei[0]; dst = ei[1]\n",
    "mask = (src < n_users) & (dst >= n_users)\n",
    "pos_u = src[mask]\n",
    "pos_i = dst[mask]\n",
    "pos_pairs = list(zip(pos_u.tolist(), pos_i.tolist()))\n",
    "print(\"num positive pairs:\", len(pos_pairs))\n",
    "\n",
    "# negative sampling helper\n",
    "all_item_globals = np.arange(n_users, n_users + n_train_items)\n",
    "\n",
    "def sample_negatives(batch_users, k=1):\n",
    "    negs = []\n",
    "    for u in batch_users:\n",
    "        choices = np.random.choice(all_item_globals, size=k, replace=True)\n",
    "        negs.append(choices)\n",
    "    return np.array(negs)\n",
    "\n",
    "# GraphSAGE encoder model\n",
    "class GraphSAGEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128, out_dim=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        if num_layers == 1:\n",
    "            self.convs.append(SAGEConv(in_dim, out_dim))\n",
    "        else:\n",
    "            self.convs.append(SAGEConv(in_dim, hidden_dim))\n",
    "            for _ in range(num_layers-2):\n",
    "                self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.convs.append(SAGEConv(hidden_dim, out_dim))\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return F.normalize(x, p=2, dim=-1)\n",
    "\n",
    "# Determina dimensão de entrada\n",
    "in_dim = X_all.shape[1]\n",
    "\n",
    "# Mover tensores constantes para DEVICE\n",
    "X_all = X_all.to(DEVICE)\n",
    "edge_index = edge_index.to(DEVICE)\n",
    "\n",
    "# --- Loop de R runs ---\n",
    "runs_summary = []\n",
    "for run in range(1, R+1):\n",
    "    # semente diferente por run\n",
    "    seed = 42 + run\n",
    "    seed_everything(seed)\n",
    "    np.random.seed(seed) \n",
    "\n",
    "    print(f\"\\n=== Run {run}/{R}  seed={seed} ===\")\n",
    "    run_ts = datetime.now().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    run_folder = os.path.join(OUT_DIR, f\"run{run}_{run_ts}\")\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "\n",
    "    # (Re)instanciar modelo e optimizer\n",
    "    encoder = GraphSAGEEncoder(in_dim, hidden_dim, out_dim, num_layers).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # treino\n",
    "    epoch_losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        random.shuffle(pos_pairs)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # processa tudo de uma vez se dataset pequeno\n",
    "        if batch_size >= len(pos_pairs):\n",
    "            batch = pos_pairs\n",
    "            batch_users = [p[0] for p in batch]\n",
    "            batch_pos_items = [p[1] for p in batch]\n",
    "            neg_items = sample_negatives(batch_users, k=1)[:,0].tolist()\n",
    "            z = encoder(X_all, edge_index)\n",
    "            u_emb = z[torch.tensor(batch_users, device=DEVICE)]\n",
    "            pos_emb = z[torch.tensor(batch_pos_items, device=DEVICE)]\n",
    "            neg_emb = z[torch.tensor(neg_items, device=DEVICE)]\n",
    "            pos_scores = (u_emb * pos_emb).sum(dim=-1)\n",
    "            neg_scores = (u_emb * neg_emb).sum(dim=-1)\n",
    "            loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-15).mean()\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            total_loss = loss.item()\n",
    "        else:\n",
    "            # mini-batch loop\n",
    "            for i in range(0, len(pos_pairs), batch_size):\n",
    "                batch = pos_pairs[i:i+batch_size]\n",
    "                batch_users = [p[0] for p in batch]\n",
    "                batch_pos_items = [p[1] for p in batch]\n",
    "                neg_items = sample_negatives(batch_users, k=1)[:,0].tolist()\n",
    "                z = encoder(X_all, edge_index)\n",
    "                u_emb = z[torch.tensor(batch_users, device=DEVICE)]\n",
    "                pos_emb = z[torch.tensor(batch_pos_items, device=DEVICE)]\n",
    "                neg_emb = z[torch.tensor(neg_items, device=DEVICE)]\n",
    "                pos_scores = (u_emb * pos_emb).sum(dim=-1)\n",
    "                neg_scores = (u_emb * neg_emb).sum(dim=-1)\n",
    "                loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-15).mean()\n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "                total_loss += loss.item() * len(batch)\n",
    "            total_loss = total_loss / len(pos_pairs)\n",
    "\n",
    "        epoch_losses.append(float(total_loss))\n",
    "        print(f\"Run {run}  Epoch {epoch:02d}/{epochs}  loss={total_loss:.6f}\")\n",
    "\n",
    "    # salvar encoder (pesos) do run\n",
    "    encoder_path = os.path.join(run_folder, f\"graphsage_encoder_run{run}.pt\")\n",
    "    torch.save(encoder.state_dict(), encoder_path)\n",
    "\n",
    "    # gerar e salvar embeddings (em CPU numpy)\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        z_all = encoder(X_all, edge_index).cpu().numpy()  # shape (n_nodes, out_dim)\n",
    "    user_embeddings = z_all[:n_users]\n",
    "    item_embeddings_train = z_all[n_users:n_users + n_train_items]\n",
    "\n",
    "    user_emb_path = os.path.join(run_folder, f\"user_embeddings_run{run}.npy\")\n",
    "    item_emb_path = os.path.join(run_folder, f\"item_embeddings_train_run{run}.npy\")\n",
    "    np.save(user_emb_path, user_embeddings)\n",
    "    np.save(item_emb_path, item_embeddings_train)\n",
    "\n",
    "    print(f\"Saved encoder and embeddings for run {run}:\")\n",
    "    print(\" - encoder:\", encoder_path)\n",
    "    print(\" - user_emb:\", user_emb_path, \"shape:\", user_embeddings.shape)\n",
    "    print(\" - item_emb (train):\", item_emb_path, \"shape:\", item_embeddings_train.shape)\n",
    "\n",
    "    # salvar resumo da run em JSON\n",
    "    run_summary = {\n",
    "        \"run\": run,\n",
    "        \"seed\": seed,\n",
    "        \"timestamp\": run_ts,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"out_dim\": out_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_positive_pairs\": len(pos_pairs),\n",
    "        \"encoder_path\": encoder_path,\n",
    "        \"user_emb_path\": user_emb_path,\n",
    "        \"item_emb_path\": item_emb_path,\n",
    "        \"epoch_losses\": epoch_losses\n",
    "    }\n",
    "    json_path = os.path.join(run_folder, f\"run_summary_{run}.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(run_summary, f, indent=2)\n",
    "    runs_summary.append(run_summary)\n",
    "\n",
    "# salvar resumo agregado de todas as runs\n",
    "agg_path = os.path.join(OUT_DIR, f\"all_runs_summary_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "with open(agg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"n_runs\": R, \"runs\": runs_summary}, f, indent=2)\n",
    "print(f\"\\nAll runs finished. Aggregate summary saved to {agg_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63688d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 state files. Generating embeddings for 297 test items each.\n",
      "\n",
      "[1/10] Run 1: loading state -> runs_graphsage\\run1_20251021T142343Z\\graphsage_encoder_run1.pt\n",
      " - checkpoint in_dim detected: 262; current train features dim: 223\n",
      " - padding features: 223 -> 262 (pad 39 zeros)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run1.npy  meta: test_embeddings_runs\\run1_meta.json\n",
      "\n",
      "[2/10] Run 1: loading state -> runs_graphsage\\run1_20251027T083914Z\\graphsage_encoder_run1.pt\n",
      " - checkpoint in_dim detected: 223; current train features dim: 223\n",
      " - truncating features: 262 -> 223 (dropping 39 cols)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run1.npy  meta: test_embeddings_runs\\run1_meta.json\n",
      "\n",
      "[3/10] Run 2: loading state -> runs_graphsage\\run2_20251021T143534Z\\graphsage_encoder_run2.pt\n",
      " - checkpoint in_dim detected: 262; current train features dim: 223\n",
      " - padding features: 223 -> 262 (pad 39 zeros)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run2.npy  meta: test_embeddings_runs\\run2_meta.json\n",
      "\n",
      "[4/10] Run 2: loading state -> runs_graphsage\\run2_20251027T233000Z\\graphsage_encoder_run2.pt\n",
      " - checkpoint in_dim detected: 223; current train features dim: 223\n",
      " - truncating features: 262 -> 223 (dropping 39 cols)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run2.npy  meta: test_embeddings_runs\\run2_meta.json\n",
      "\n",
      "[5/10] Run 3: loading state -> runs_graphsage\\run3_20251021T144658Z\\graphsage_encoder_run3.pt\n",
      " - checkpoint in_dim detected: 262; current train features dim: 223\n",
      " - padding features: 223 -> 262 (pad 39 zeros)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run3.npy  meta: test_embeddings_runs\\run3_meta.json\n",
      "\n",
      "[6/10] Run 3: loading state -> runs_graphsage\\run3_20251028T142312Z\\graphsage_encoder_run3.pt\n",
      " - checkpoint in_dim detected: 223; current train features dim: 223\n",
      " - truncating features: 262 -> 223 (dropping 39 cols)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run3.npy  meta: test_embeddings_runs\\run3_meta.json\n",
      "\n",
      "[7/10] Run 4: loading state -> runs_graphsage\\run4_20251021T145840Z\\graphsage_encoder_run4.pt\n",
      " - checkpoint in_dim detected: 262; current train features dim: 223\n",
      " - padding features: 223 -> 262 (pad 39 zeros)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run4.npy  meta: test_embeddings_runs\\run4_meta.json\n",
      "\n",
      "[8/10] Run 4: loading state -> runs_graphsage\\run4_20251029T073740Z\\graphsage_encoder_run4.pt\n",
      " - checkpoint in_dim detected: 223; current train features dim: 223\n",
      " - truncating features: 262 -> 223 (dropping 39 cols)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run4.npy  meta: test_embeddings_runs\\run4_meta.json\n",
      "\n",
      "[9/10] Run 5: loading state -> runs_graphsage\\run5_20251021T151011Z\\graphsage_encoder_run5.pt\n",
      " - checkpoint in_dim detected: 262; current train features dim: 223\n",
      " - padding features: 223 -> 262 (pad 39 zeros)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run5.npy  meta: test_embeddings_runs\\run5_meta.json\n",
      "\n",
      "[10/10] Run 5: loading state -> runs_graphsage\\run5_20251029T235944Z\\graphsage_encoder_run5.pt\n",
      " - checkpoint in_dim detected: 223; current train features dim: 223\n",
      " - truncating features: 262 -> 223 (dropping 39 cols)\n",
      " Saved: test_embeddings_runs\\test_item_embeddings_run5.npy  meta: test_embeddings_runs\\run5_meta.json\n",
      "\n",
      "Finished: saved embeddings for 10 runs in test_embeddings_runs\n"
     ]
    }
   ],
   "source": [
    "# Configurações\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "STATE_DIR = \"runs_graphsage\"\n",
    "STATE_PATTERN = os.path.join(STATE_DIR, \"**\", \"graphsage_encoder_run*.pt\")\n",
    "OUT_DIR = \"test_embeddings_runs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Artefatos de entrada (existem no seu ambiente)\n",
    "X_all_train = torch.load(\"X_all_train.pt\")           # (n_nodes_train, D)\n",
    "edge_index = torch.load(\"edge_index_train.pt\")       # (2, E)\n",
    "with open(\"dims.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dims = json.load(f)\n",
    "D = int(dims[\"feature_dim\"])\n",
    "n_nodes_train = X_all_train.shape[0]\n",
    "\n",
    "# Verificações obrigatórias (assuma definidas)\n",
    "try:\n",
    "    test_item_ids\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Variável `test_item_ids` não encontrada. Defina a lista de ids de itens de teste antes de rodar.\")\n",
    "\n",
    "try:\n",
    "    transform_test_items_batch\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Função `transform_test_items_batch` não encontrada. Importe/defina-a antes de rodar.\")\n",
    "\n",
    "# Construir X_test_full (numpy) e checar dimensão básica\n",
    "X_test_full = transform_test_items_batch([ item_features_raw.get(str(iid), []) for iid in test_item_ids ], save_path=None)\n",
    "if X_test_full.shape[1] != D:\n",
    "    raise RuntimeError(f\"Dimensão de features dos itens de teste ({X_test_full.shape[1]}) difere de D ({D}). Ajuste transform_test_items_batch/feature padding.\")\n",
    "\n",
    "n_test = X_test_full.shape[0]\n",
    "\n",
    "# Preparar array estendido X_all_ext_t (numpy por enquanto)\n",
    "X_all_extended = np.vstack([ X_all_train.cpu().numpy(), X_test_full ])   # (n_nodes_train + n_test, D)\n",
    "\n",
    "# edge_index para device\n",
    "edge_index = edge_index.to(DEVICE)\n",
    "\n",
    "# Obter lista de arquivos de estado do encoder\n",
    "state_paths = sorted(glob.glob(STATE_PATTERN, recursive=True))\n",
    "if len(state_paths) == 0:\n",
    "    raise RuntimeError(f\"Nenhum arquivo de estado localizado com o padrão: {STATE_PATTERN}. Ajuste STATE_DIR/STATE_PATTERN.\")\n",
    "\n",
    "print(f\"Found {len(state_paths)} state files. Generating embeddings for {n_test} test items each.\")\n",
    "\n",
    "# Função utilitária para extrair run number\n",
    "def extract_run_number(path):\n",
    "    base = os.path.basename(path)\n",
    "    m = re.search(r\"run(\\d+)\", base)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "state_paths_sorted = sorted(state_paths, key=lambda p: (extract_run_number(p) or 0, p))\n",
    "\n",
    "runs_info = []\n",
    "for idx, sp in enumerate(state_paths_sorted, start=1):\n",
    "    run_num = extract_run_number(sp) or idx\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    print(f\"\\n[{idx}/{len(state_paths_sorted)}] Run {run_num}: loading state -> {sp}\")\n",
    "\n",
    "    # Carregar checkpoint (map to cpu para inspeção)\n",
    "    ckpt = torch.load(sp, map_location='cpu')\n",
    "\n",
    "    # Extrair state_dict real (se checkpoint for um dict com chaves como 'state_dict' ou 'model_state_dict')\n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'state_dict' in ckpt:\n",
    "            state_dict = ckpt['state_dict']\n",
    "        elif 'model_state_dict' in ckpt:\n",
    "            state_dict = ckpt['model_state_dict']\n",
    "        else:\n",
    "            # assume ckpt já é o state_dict\n",
    "            state_dict = ckpt\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "\n",
    "    # Tentar detectar a chave dos pesos de entrada da primeira conv (lin_l.weight preferencialmente)\n",
    "    in_dim_ckpt = None\n",
    "    candidate_keys = []\n",
    "    for k in state_dict.keys():\n",
    "        if k.endswith('lin_l.weight') or k.endswith('lin_r.weight'):\n",
    "            candidate_keys.append(k)\n",
    "    if candidate_keys:\n",
    "        # preferir lin_l.weight da primeira camada (convs.0.lin_l.weight)\n",
    "        preferred = None\n",
    "        for k in candidate_keys:\n",
    "            if re.search(r'convs\\.0\\..*lin_l.weight', k):\n",
    "                preferred = k\n",
    "                break\n",
    "        if preferred is None:\n",
    "            preferred = candidate_keys[0]\n",
    "        in_dim_ckpt = state_dict[preferred].shape[1]\n",
    "    else:\n",
    "        # fallback: pegar primeira weight linear conv / linear qualquer\n",
    "        for k, v in state_dict.items():\n",
    "            if v.ndim == 2:\n",
    "                in_dim_ckpt = v.shape[1]\n",
    "                break\n",
    "\n",
    "    if in_dim_ckpt is None:\n",
    "        raise RuntimeError(\"Não foi possível inferir in_dim a partir do checkpoint. Verifique o conteúdo do arquivo de estado.\")\n",
    "\n",
    "    print(f\" - checkpoint in_dim detected: {in_dim_ckpt}; current train features dim: {X_all_train.shape[1]}\")\n",
    "\n",
    "    # Ajustar X_all_extended (pad/truncate) para casar com in_dim_ckpt\n",
    "    cur_dim = X_all_extended.shape[1]\n",
    "    if cur_dim < in_dim_ckpt:\n",
    "        pad_width = in_dim_ckpt - cur_dim\n",
    "        print(f\" - padding features: {cur_dim} -> {in_dim_ckpt} (pad {pad_width} zeros)\")\n",
    "        X_all_extended = np.hstack([X_all_extended, np.zeros((X_all_extended.shape[0], pad_width), dtype=X_all_extended.dtype)])\n",
    "    elif cur_dim > in_dim_ckpt:\n",
    "        print(f\" - truncating features: {cur_dim} -> {in_dim_ckpt} (dropping {cur_dim - in_dim_ckpt} cols)\")\n",
    "        X_all_extended = X_all_extended[:, :in_dim_ckpt]\n",
    "    else:\n",
    "        print(\" - feature dimension matches checkpoint; no pad/truncate needed\")\n",
    "\n",
    "    # Converter para tensor no DEVICE\n",
    "    X_all_ext_t = torch.tensor(X_all_extended, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "    # Instanciar encoder com in_dim do checkpoint (manter demais hiperparâmetros como no treino)\n",
    "    # Ajuste hidden_dim/out_dim/num_layers se diferente do treino original\n",
    "    encoder = GraphSAGEEncoder(in_dim=in_dim_ckpt, hidden_dim=64, out_dim=32, num_layers=1).to(DEVICE)\n",
    "\n",
    "    # Tentar carregar state_dict; se falhar por chaves inesperadas, tentar load strict=False e depois reportar\n",
    "    try:\n",
    "        encoder.load_state_dict(state_dict)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Warning: full load_state_dict failed, trying strict=False. Error:\", e)\n",
    "        missing, unexpected = encoder.load_state_dict(state_dict, strict=False)\n",
    "        print(\" - missing keys:\", missing)\n",
    "        print(\" - unexpected keys:\", unexpected)\n",
    "        # se desejar, pode inicializar manualmente pesos faltantes aqui (opcional)\n",
    "\n",
    "    encoder.eval()\n",
    "\n",
    "    # Forward para todos os nós estendidos (usa edge_index do treino)\n",
    "    with torch.no_grad():\n",
    "        emb_all = encoder(X_all_ext_t, edge_index)   # tensor (n_nodes_train + n_test, out_dim)\n",
    "    emb_all_np = emb_all.cpu().numpy()\n",
    "\n",
    "    test_global_start = n_nodes_train\n",
    "    test_global_indices = np.arange(test_global_start, test_global_start + n_test)\n",
    "    test_embeddings = emb_all_np[test_global_indices, :]\n",
    "\n",
    "    # Salvar embeddings por run\n",
    "    out_emb_path = os.path.join(OUT_DIR, f\"test_item_embeddings_run{run_num}.npy\")\n",
    "    np.save(out_emb_path, test_embeddings)\n",
    "\n",
    "    # Salvar mapping id->index (salva apenas uma vez por segurança; sobrescreve se já existir)\n",
    "    mapping = { str(test_item_ids[i]): int(i) for i in range(len(test_item_ids)) }\n",
    "    mapping_path = os.path.join(OUT_DIR, \"test_item_id2idx.json\")\n",
    "    with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Salvar metadados da run\n",
    "    run_meta = {\n",
    "        \"run_index_in_list\": idx,\n",
    "        \"run_number_extracted\": run_num,\n",
    "        \"state_path\": sp,\n",
    "        \"embeddings_path\": out_emb_path,\n",
    "        \"n_test_items\": n_test,\n",
    "        \"timestamp_utc\": ts\n",
    "    }\n",
    "    meta_path = os.path.join(OUT_DIR, f\"run{run_num}_meta.json\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(run_meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\" Saved: {out_emb_path}  meta: {meta_path}\")\n",
    "    runs_info.append(run_meta)\n",
    "\n",
    "# Salvar resumo agregado\n",
    "agg_path = os.path.join(OUT_DIR, f\"all_runs_test_embeddings_summary_{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "with open(agg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"n_states\": len(state_paths_sorted), \"runs\": runs_info}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nFinished: saved embeddings for {len(runs_info)} runs in {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "315e4678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test_item_embeddings_run1.npy -> label run1\n",
      " Saved top-K indices to test_embeddings_runs\\topk_per_run\\run1_topk_user_indices.npy and JSON test_embeddings_runs\\topk_per_run\\run1_topk_user_indices.json\n",
      "\n",
      "Processing test_item_embeddings_run2.npy -> label run2\n",
      " Saved top-K indices to test_embeddings_runs\\topk_per_run\\run2_topk_user_indices.npy and JSON test_embeddings_runs\\topk_per_run\\run2_topk_user_indices.json\n",
      "\n",
      "Processing test_item_embeddings_run3.npy -> label run3\n",
      " Saved top-K indices to test_embeddings_runs\\topk_per_run\\run3_topk_user_indices.npy and JSON test_embeddings_runs\\topk_per_run\\run3_topk_user_indices.json\n",
      "\n",
      "Processing test_item_embeddings_run4.npy -> label run4\n",
      " Saved top-K indices to test_embeddings_runs\\topk_per_run\\run4_topk_user_indices.npy and JSON test_embeddings_runs\\topk_per_run\\run4_topk_user_indices.json\n",
      "\n",
      "Processing test_item_embeddings_run5.npy -> label run5\n",
      " Saved top-K indices to test_embeddings_runs\\topk_per_run\\run5_topk_user_indices.npy and JSON test_embeddings_runs\\topk_per_run\\run5_topk_user_indices.json\n",
      "\n",
      "Finished processing 5 run(s). Summary: test_embeddings_runs\\topk_per_run\\all_runs_topk_summary_20251030T230527Z.json\n"
     ]
    }
   ],
   "source": [
    "# Carregar artefatos, construir ground truth sets por item e buscar\n",
    "# top-50 users por similaridade para cada item de teste, em cada run\n",
    "\n",
    "# Configurações\n",
    "EMB_DIR = \"test_embeddings_runs\"\n",
    "PATTERN = os.path.join(EMB_DIR, \"test_item_embeddings_run*.npy\")\n",
    "OUT_DIR = os.path.join(EMB_DIR, \"topk_per_run\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "K_max = 50\n",
    "USER_EMB_PATH = \"user_embeddings.npy\"\n",
    "TEST_ID2ROW_PATH = os.path.join(EMB_DIR, \"test_item_id2idx.json\") \n",
    "\n",
    "# Verificações iniciais\n",
    "state_files = sorted(glob.glob(PATTERN))\n",
    "if len(state_files) == 0:\n",
    "    raise RuntimeError(f\"Nenhum arquivo de embeddings encontrado com o padrão: {PATTERN}\")\n",
    "\n",
    "if not os.path.isfile(USER_EMB_PATH):\n",
    "    raise RuntimeError(f\"Arquivo de embeddings de usuários não encontrado: {USER_EMB_PATH}\")\n",
    "\n",
    "if not os.path.isfile(TEST_ID2ROW_PATH):\n",
    "    raise RuntimeError(f\"Arquivo de mapeamento test_item_id2idx não encontrado: {TEST_ID2ROW_PATH}\")\n",
    "\n",
    "# Carregar recursos comuns\n",
    "U = np.load(USER_EMB_PATH)              # shape (n_users, d)\n",
    "with open(TEST_ID2ROW_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_id2row = json.load(f)          # map item_id -> row_index\n",
    "\n",
    "# Normalização L2 das linhas (segura para zero-vectors)\n",
    "def l2_normalize_rows(X):\n",
    "    X = X.astype(np.float64)\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    return (X / norms)\n",
    "\n",
    "U_norm = l2_normalize_rows(U)           # (n_users, d)\n",
    "n_users = U_norm.shape[0]\n",
    "\n",
    "runs_summary = []\n",
    "for emb_path in state_files:\n",
    "    # Extrair run id (nome do arquivo) para rotular saídas\n",
    "    base = os.path.basename(emb_path)\n",
    "    # ex: test_item_embeddings_run3.npy -> run3\n",
    "    run_label = base.replace(\"test_item_embeddings_\", \"\").replace(\".npy\",\"\")\n",
    "    out_prefix = os.path.join(OUT_DIR, run_label)\n",
    "\n",
    "    print(f\"\\nProcessing {base} -> label {run_label}\")\n",
    "    I = np.load(emb_path)                 # shape (n_test, d)\n",
    "    if I.ndim != 2:\n",
    "        raise RuntimeError(f\"Embeddings file {emb_path} tem dimensão inválida: {I.shape}\")\n",
    "\n",
    "    # normalizar\n",
    "    I_norm = l2_normalize_rows(I)\n",
    "\n",
    "    n_test = I_norm.shape[0]\n",
    "    if I_norm.shape[1] != U_norm.shape[1]:\n",
    "        raise RuntimeError(f\"Dimensionalidade incompatível: item emb dim {I_norm.shape[1]} != user emb dim {U_norm.shape[1]}\")\n",
    "\n",
    "    # calcular similaridades em blocos se necessário (para controlar memória)\n",
    "    # sims será evitado em memória completa se for grande; processaremos item-por-item em loop com argpartition\n",
    "    topk_indices = np.zeros((n_test, min(K_max, n_users)), dtype=np.int32)\n",
    "    topk_scores = np.zeros((n_test, min(K_max, n_users)), dtype=np.float32)\n",
    "\n",
    "    # escolha de processamento: se n_test * n_users razoável, faça produto matriz inteiro; senão, calcule por batch\n",
    "    mem_cost = n_test * n_users\n",
    "    threshold_full_mat = 200_000_000  # heurística: evitar criar matriz maior que ~200M elementos (ajuste conforme memória)\n",
    "    if mem_cost <= threshold_full_mat:\n",
    "        sims = I_norm.dot(U_norm.T)  # (n_test, n_users)\n",
    "        for i in range(n_test):\n",
    "            row = sims[i]\n",
    "            k = min(K_max, n_users)\n",
    "            if n_users <= k:\n",
    "                idx_sorted = np.argsort(-row)\n",
    "            else:\n",
    "                idx_part = np.argpartition(-row, k-1)[:k]\n",
    "                idx_sorted = idx_part[np.argsort(-row[idx_part])]\n",
    "            topk_indices[i, :len(idx_sorted)] = idx_sorted\n",
    "            topk_scores[i, :len(idx_sorted)] = row[idx_sorted]\n",
    "        # liberar memória\n",
    "        del sims\n",
    "    else:\n",
    "        # processamento em batches de itens para economizar memória\n",
    "        batch_size_items = max(1, int(1e6 // n_users))  # heurística simples para batch size\n",
    "        for start in range(0, n_test, batch_size_items):\n",
    "            end = min(n_test, start + batch_size_items)\n",
    "            batch = I_norm[start:end]              # (b, d)\n",
    "            sims_batch = batch.dot(U_norm.T)       # (b, n_users)\n",
    "            for bi in range(end - start):\n",
    "                row = sims_batch[bi]\n",
    "                k = min(K_max, n_users)\n",
    "                if n_users <= k:\n",
    "                    idx_sorted = np.argsort(-row)\n",
    "                else:\n",
    "                    idx_part = np.argpartition(-row, k-1)[:k]\n",
    "                    idx_sorted = idx_part[np.argsort(-row[idx_part])]\n",
    "                topk_indices[start + bi, :len(idx_sorted)] = idx_sorted\n",
    "                topk_scores[start + bi, :len(idx_sorted)] = row[idx_sorted]\n",
    "            del sims_batch\n",
    "\n",
    "    # Salvar resultados por run\n",
    "    topk_idx_path = out_prefix + \"_topk_user_indices.npy\"   # shape (n_test, K')\n",
    "    topk_sc_path = out_prefix + \"_topk_scores.npy\"\n",
    "    np.save(topk_idx_path, topk_indices)\n",
    "    np.save(topk_sc_path, topk_scores)\n",
    "\n",
    "    # Salvar também em JSON compacto: lista de listas de int (user indices) — pode ser grande; verificar necessidade\n",
    "    json_list = [ list(map(int, topk_indices[i])) for i in range(n_test) ]\n",
    "    json_path = out_prefix + \"_topk_user_indices.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_list, f, ensure_ascii=False)\n",
    "\n",
    "    run_info = {\n",
    "        \"emb_path\": emb_path,\n",
    "        \"run_label\": run_label,\n",
    "        \"n_test_items\": int(n_test),\n",
    "        \"n_users\": int(n_users),\n",
    "        \"k_requested\": int(K_max),\n",
    "        \"topk_idx_npy\": topk_idx_path,\n",
    "        \"topk_scores_npy\": topk_sc_path,\n",
    "        \"topk_idx_json\": json_path,\n",
    "        \"timestamp\": datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    }\n",
    "    runs_summary.append(run_info)\n",
    "    print(f\" Saved top-K indices to {topk_idx_path} and JSON {json_path}\")\n",
    "\n",
    "# salvar resumo agregado\n",
    "summary_path = os.path.join(OUT_DIR, f\"all_runs_topk_summary_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"n_runs\": len(runs_summary), \"runs\": runs_summary}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nFinished processing {len(runs_summary)} run(s). Summary: {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33f1fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed gt_item2users.\n",
      "n_test_items_total (mapping): 297\n",
      "n_test_items_with_positives (evaluated_items): 297\n"
     ]
    }
   ],
   "source": [
    "# Reconstruir gt_item2users a partir dos artefatos existentes\n",
    "\n",
    "# Ajuste os nomes de arquivo/variáveis se necessário\n",
    "# Supõe:\n",
    "# - test_interactions: DataFrame com colunas 'user_id' e 'item_id' (strings ou numéricos)\n",
    "# - user2idx: dict mapping user_id (string) -> user_index (int)\n",
    "# - test_item_id2idx.json ou test_item_id2idx já carregado como test_id2row\n",
    "\n",
    "# Carregar mapping test_id2row se ainda não existir\n",
    "try:\n",
    "    test_id2row\n",
    "except NameError:\n",
    "    # tenta carregar do JSON produzido anteriormente\n",
    "    import os\n",
    "    if os.path.isfile(\"test_item_id2idx.json\"):\n",
    "        with open(\"test_item_id2idx.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "            test_id2row = json.load(f)\n",
    "    else:\n",
    "        raise RuntimeError(\"test_id2row não encontrado e arquivo test_item_id2idx.json também não existe\")\n",
    "\n",
    "# Garantir tipos string nas chaves para consistência\n",
    "test_id2row = { str(k): int(v) for k,v in test_id2row.items() }\n",
    "\n",
    "# Garantir user2idx existe\n",
    "try:\n",
    "    user2idx\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Variável user2idx não encontrada no ambiente. Defina user2idx (mapping user_id -> user_index) antes.\")\n",
    "\n",
    "# Normalizar tipos no DataFrame test_interactions\n",
    "test_interactions['user_id'] = test_interactions['user_id'].astype(str)\n",
    "test_interactions['item_id'] = test_interactions['item_id'].astype(str)\n",
    "\n",
    "# Construir gt_item2users: item_id (string) -> set of user indices (ints)\n",
    "gt_item2users = {}\n",
    "for _, row in test_interactions.iterrows():\n",
    "    iid = str(row['item_id'])\n",
    "    uid = str(row['user_id'])\n",
    "    if iid in test_id2row and uid in user2idx:\n",
    "        uidx = int(user2idx[uid])\n",
    "        gt_item2users.setdefault(iid, set()).add(uidx)\n",
    "\n",
    "# Opcional: garantir que todos os test_item_ids estejam presentes como chaves (mesmo que vazio)\n",
    "try:\n",
    "    test_item_ids\n",
    "except NameError:\n",
    "    # construir list a partir do mapping de ids\n",
    "    test_item_ids = sorted(list(test_id2row.keys()), key=lambda x: int(x) if x.isdigit() else x)\n",
    "\n",
    "for iid in test_item_ids:\n",
    "    gt_item2users.setdefault(str(iid), set())\n",
    "\n",
    "# Construir evaluated_items (itens com pelo menos um positivo)\n",
    "evaluated_items = [iid for iid, s in gt_item2users.items() if len(s) > 0]\n",
    "\n",
    "print(\"Reconstructed gt_item2users.\")\n",
    "print(\"n_test_items_total (mapping):\", len(test_id2row))\n",
    "print(\"n_test_items_with_positives (evaluated_items):\", len(evaluated_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9842e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: test_item_ids tem 297 itens (esperado 30). O código continuará usando essa lista.\n",
      "Found 5 run files. Using JSON format: True\n",
      "Processing run 1/5: run1_topk_user_indices.json\n",
      "Processing run 2/5: run2_topk_user_indices.json\n",
      "Processing run 3/5: run3_topk_user_indices.json\n",
      "Processing run 4/5: run4_topk_user_indices.json\n",
      "Processing run 5/5: run5_topk_user_indices.json\n",
      "\n",
      "Saved aggregated metrics per item across 5 runs to: test_embeddings_runs/topk_per_run\\aggregated_precision_ndcg_per_item_across_runs.json\n",
      "Each list contains 297 elements (None indicates item had no positives in all runs).\n"
     ]
    }
   ],
   "source": [
    "# Agregar Precision@K e NDCG@K por item entre runs\n",
    "\n",
    "# Configurações\n",
    "TOPK_RUNS_DIR = \"test_embeddings_runs/topk_per_run\"   # local onde topk por run foram salvos\n",
    "PATTERN_JSON = os.path.join(TOPK_RUNS_DIR, \"*_topk_user_indices.json\")\n",
    "# alternativa: carregar .npy se JSONs não existirem (arquivo *_topk_user_indices.npy)\n",
    "PATTERN_NPY = os.path.join(TOPK_RUNS_DIR, \"*_topk_user_indices.npy\")\n",
    "\n",
    "Ks = [10, 20, 50]\n",
    "# ordem dos 30 itens — assegure que test_item_ids esteja definido e contenha 30 ids strings\n",
    "try:\n",
    "    test_item_ids\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Variável `test_item_ids` não encontrada. Defina-a como a lista (30) de ids de itens de teste na ordem desejada.\")\n",
    "\n",
    "n_items_expected = len(test_item_ids)\n",
    "if n_items_expected != 30:\n",
    "    print(f\"Warning: test_item_ids tem {n_items_expected} itens (esperado 30). O código continuará usando essa lista.\")\n",
    "\n",
    "# Carregar arquivos disponíveis (prefere JSON, senão NPZ/NPY)\n",
    "files_json = sorted(glob.glob(PATTERN_JSON))\n",
    "files_npy = sorted(glob.glob(PATTERN_NPY))\n",
    "\n",
    "if len(files_json) == 0 and len(files_npy) == 0:\n",
    "    raise RuntimeError(f\"Nenhum arquivo de top-K encontrado em {TOPK_RUNS_DIR} com padrões JSON/NPY.\")\n",
    "\n",
    "run_files = []\n",
    "use_json = False\n",
    "if len(files_json) > 0:\n",
    "    run_files = files_json\n",
    "    use_json = True\n",
    "else:\n",
    "    run_files = files_npy\n",
    "    use_json = False\n",
    "\n",
    "n_runs = len(run_files)\n",
    "print(f\"Found {n_runs} run files. Using JSON format: {use_json}\")\n",
    "\n",
    "# Funções de métricas\n",
    "def precision_at_k_from_list(recommended, gt_set, k):\n",
    "    rec_k = recommended[:k]\n",
    "    hits = sum(1 for u in rec_k if u in gt_set)\n",
    "    return hits / k\n",
    "\n",
    "def dcg_at_k_from_list(recommended, gt_set, k):\n",
    "    rec_k = recommended[:k]\n",
    "    dcg = 0.0\n",
    "    for i, u in enumerate(rec_k):\n",
    "        rel = 1.0 if u in gt_set else 0.0\n",
    "        dcg += rel / log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def idcg_at_k_from_gt(gt_set, k):\n",
    "    n_rel = min(len(gt_set), k)\n",
    "    if n_rel == 0:\n",
    "        return 0.0\n",
    "    return sum(1.0 / log2(i + 2) for i in range(n_rel))\n",
    "\n",
    "# Inicializar arrays para acumular métricas por run por item\n",
    "# shape: (n_runs, n_items)\n",
    "n_items = n_items_expected\n",
    "prec_by_run = {k: np.zeros((n_runs, n_items), dtype=float) for k in Ks}\n",
    "ndcg_by_run = {k: np.zeros((n_runs, n_items), dtype=float) for k in Ks}\n",
    "\n",
    "# Iterar runs e preencher matrizes\n",
    "for r_idx, fpath in enumerate(run_files):\n",
    "    print(f\"Processing run {r_idx+1}/{n_runs}: {os.path.basename(fpath)}\")\n",
    "    if use_json:\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            topk_list_per_item = json.load(f)   # espera lista (n_test) de listas de user indices\n",
    "        # topk_list_per_item indexed by row index 0..n_test-1\n",
    "    else:\n",
    "        arr = np.load(fpath)   # shape (n_test, K')\n",
    "        # converter para lista de lists\n",
    "        topk_list_per_item = [ list(map(int, arr[i])) for i in range(arr.shape[0]) ]\n",
    "\n",
    "    # Checagem de consistência\n",
    "    n_test_runs = len(topk_list_per_item)\n",
    "    if n_test_runs < n_items:\n",
    "        print(f\"Warning: run file possui apenas {n_test_runs} itens, menor que n_items={n_items}. Preenchendo faltantes com listas vazias.\")\n",
    "        # expandir com listas vazias\n",
    "        topk_list_per_item += [[] for _ in range(n_items - n_test_runs)]\n",
    "    elif n_test_runs > n_items:\n",
    "        # possível que o arquivo tenha todos os itens, mas test_item_ids refere 30; apenas use os primeiros n_items\n",
    "        pass\n",
    "\n",
    "    # Para cada item na ordem test_item_ids, obter sua row index e as recomendações\n",
    "    for j, iid in enumerate(test_item_ids):\n",
    "        # obter row index no arquivo topk (assume mapping test_item_id2idx usado antes)\n",
    "        try:\n",
    "            row_idx = int(test_id2row[str(iid)])\n",
    "        except Exception:\n",
    "            # se não encontrou mapping, assumimos que a order em topk_list_per_item segue test_item_ids\n",
    "            row_idx = j\n",
    "\n",
    "        if row_idx >= len(topk_list_per_item):\n",
    "            recommended = []\n",
    "        else:\n",
    "            recommended = topk_list_per_item[row_idx]\n",
    "            # garantir que todos os elementos sejam ints\n",
    "            recommended = [int(x) for x in recommended]\n",
    "\n",
    "        gt_set = gt_item2users.get(str(iid), set())\n",
    "        # Se gt_set vazio, definimos métricas como NaN e iremos ignorar na média final\n",
    "        if len(gt_set) == 0:\n",
    "            for k in Ks:\n",
    "                prec_by_run[k][r_idx, j] = np.nan\n",
    "                ndcg_by_run[k][r_idx, j] = np.nan\n",
    "            continue\n",
    "\n",
    "        # calcular métricas para cada K\n",
    "        for k in Ks:\n",
    "            # se recommended menor que k, pad com valores que não estão no gt (ex: -1)\n",
    "            if len(recommended) < k:\n",
    "                rec_padded = recommended + [-1] * (k - len(recommended))\n",
    "            else:\n",
    "                rec_padded = recommended\n",
    "            prec = precision_at_k_from_list(rec_padded, gt_set, k)\n",
    "            dcg = dcg_at_k_from_list(rec_padded, gt_set, k)\n",
    "            idcg = idcg_at_k_from_gt(gt_set, k)\n",
    "            ndcg = (dcg / idcg) if idcg > 0 else 0.0\n",
    "            prec_by_run[k][r_idx, j] = prec\n",
    "            ndcg_by_run[k][r_idx, j] = ndcg\n",
    "\n",
    "# Agora calcular média por item entre runs, ignorando NaNs (runs onde item tinha 0 positivos)\n",
    "avg_prec_per_item = {k: [] for k in Ks}\n",
    "avg_ndcg_per_item = {k: [] for k in Ks}\n",
    "for j in range(n_items):\n",
    "    for k in Ks:\n",
    "        col = prec_by_run[k][:, j]\n",
    "        # ignorar NaNs\n",
    "        valid = col[~np.isnan(col)]\n",
    "        if valid.size == 0:\n",
    "            avg = None\n",
    "        else:\n",
    "            avg = float(np.mean(valid))\n",
    "        avg_prec_per_item[k].append(avg)\n",
    "\n",
    "        col_ndcg = ndcg_by_run[k][:, j]\n",
    "        valid_ndcg = col_ndcg[~np.isnan(col_ndcg)]\n",
    "        if valid_ndcg.size == 0:\n",
    "            avgn = None\n",
    "        else:\n",
    "            avgn = float(np.mean(valid_ndcg))\n",
    "        avg_ndcg_per_item[k].append(avgn)\n",
    "\n",
    "# Construir listas solicitadas (6 listas com 30 elementos cada, na ordem test_item_ids)\n",
    "precision_at_10 = avg_prec_per_item[10]\n",
    "precision_at_20 = avg_prec_per_item[20]\n",
    "precision_at_50 = avg_prec_per_item[50]\n",
    "ndcg_at_10 = avg_ndcg_per_item[10]\n",
    "ndcg_at_20 = avg_ndcg_per_item[20]\n",
    "ndcg_at_50 = avg_ndcg_per_item[50]\n",
    "\n",
    "# Checagem de tamanhos\n",
    "assert len(precision_at_10) == n_items\n",
    "assert len(ndcg_at_50) == n_items\n",
    "\n",
    "# Salvar em JSON\n",
    "out_result = {\n",
    "    \"test_item_ids_order\": test_item_ids,\n",
    "    \"precision_at_10\": precision_at_10,\n",
    "    \"precision_at_20\": precision_at_20,\n",
    "    \"precision_at_50\": precision_at_50,\n",
    "    \"ndcg_at_10\": ndcg_at_10,\n",
    "    \"ndcg_at_20\": ndcg_at_20,\n",
    "    \"ndcg_at_50\": ndcg_at_50\n",
    "}\n",
    "\n",
    "OUT_JSON = os.path.join(TOPK_RUNS_DIR, \"aggregated_precision_ndcg_per_item_across_runs.json\")\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out_result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSaved aggregated metrics per item across {n_runs} runs to: {OUT_JSON}\")\n",
    "print(f\"Each list contains {len(test_item_ids)} elements (None indicates item had no positives in all runs).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea5bcea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo das médias (ignora itens sem valor):\n",
      " - precision_at_10: mean=0.07185185185185186, valid_items=297/297\n",
      " - precision_at_20: mean=0.07336700336700337, valid_items=297/297\n",
      " - precision_at_50: mean=0.07746801346801349, valid_items=297/297\n",
      " - ndcg_at_10: mean=0.06977482515473928, valid_items=297/297\n",
      " - ndcg_at_20: mean=0.07137593734248229, valid_items=297/297\n",
      " - ndcg_at_50: mean=0.07547874832934061, valid_items=297/297\n"
     ]
    }
   ],
   "source": [
    "# caminho do arquivo\n",
    "IN_PATH = os.path.join(\"test_embeddings_runs\", \"topk_per_run\", \"aggregated_precision_ndcg_per_item_across_runs.json\")\n",
    "OUT_PATH = os.path.join(\"test_embeddings_runs\", \"topk_per_run\", \"summary_mean_precision_ndcg.json\")\n",
    "\n",
    "if not os.path.isfile(IN_PATH):\n",
    "    raise RuntimeError(f\"Arquivo não encontrado: {IN_PATH}\")\n",
    "\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# nomes esperados no JSON: precision_at_10, precision_at_20, precision_at_50, ndcg_at_10, ndcg_at_20, ndcg_at_50\n",
    "keys = [\"precision_at_10\", \"precision_at_20\", \"precision_at_50\", \"ndcg_at_10\", \"ndcg_at_20\", \"ndcg_at_50\"]\n",
    "\n",
    "result_means = {}\n",
    "for k in keys:\n",
    "    lst = data.get(k)\n",
    "    if lst is None:\n",
    "        result_means[k] = None\n",
    "        continue\n",
    "    # converter valores None para np.nan e calcular média ignorando np.nan\n",
    "    arr = np.array([np.nan if v is None else float(v) for v in lst], dtype=float)\n",
    "    # contar elementos válidos\n",
    "    valid_mask = ~np.isnan(arr)\n",
    "    valid_count = int(valid_mask.sum())\n",
    "    if valid_count == 0:\n",
    "        mean_val = None\n",
    "    else:\n",
    "        mean_val = float(np.nanmean(arr))\n",
    "    result_means[k] = {\"mean\": mean_val, \"n_valid_items\": valid_count, \"n_total_items\": int(len(arr))}\n",
    "\n",
    "# imprimir resumo\n",
    "print(\"Resumo das médias (ignora itens sem valor):\")\n",
    "for k, v in result_means.items():\n",
    "    print(f\" - {k}: mean={v['mean']}, valid_items={v['n_valid_items']}/{v['n_total_items']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
