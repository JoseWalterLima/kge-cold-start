{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b39d41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "import optuna\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Default path to data files\n",
    "PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccec6192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0      196      242       3\n",
       "1      186      302       3\n",
       "2       22      377       1\n",
       "3      244       51       2\n",
       "4      166      346       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load user-item interaction data\n",
    "interaction_data = pd.read_csv(\n",
    "    PATH + 'ml-100k/u.data',\n",
    "    sep='\\t',\n",
    "    encoding=\"latin1\",\n",
    "    names=['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    )[['user_id', 'item_id', 'rating']]\n",
    "display(interaction_data.shape)\n",
    "interaction_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db06c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', ['age:24', 'gender:M', 'occupation:technician', 'zipcode:85'])\n",
      "('2', ['age:53', 'gender:F', 'occupation:other', 'zipcode:94'])\n",
      "('3', ['age:23', 'gender:M', 'occupation:writer', 'zipcode:32'])\n",
      "('4', ['age:24', 'gender:M', 'occupation:technician', 'zipcode:43'])\n",
      "('5', ['age:33', 'gender:F', 'occupation:other', 'zipcode:15'])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a defaultdict to hold user features\n",
    "user_data = defaultdict(dict)\n",
    "\n",
    "# Read data and build user features dictionary\n",
    "def load_feature(file_path, feature_name):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            userId = row['userId']\n",
    "            value = row[feature_name]\n",
    "            user_data[userId][feature_name] = value\n",
    "\n",
    "# Load each feature file\n",
    "load_feature(PATH + 'ageRel.csv', 'age')\n",
    "load_feature(PATH + 'genderRel.csv', 'gender')\n",
    "load_feature(PATH + 'occupationRel.csv', 'occupation')\n",
    "load_feature(PATH + 'residesRel.csv', 'zipcode')\n",
    "\n",
    "# Build user features list\n",
    "user_features_raw = [\n",
    "    (userId, [f'age:{data[\"age\"]}', f'gender:{data[\"gender\"]}', f'occupation:{data[\"occupation\"]}', f'zipcode:{data[\"zipcode\"]}'])\n",
    "    for userId, data in user_data.items()\n",
    "]\n",
    "\n",
    "# Display first 5 user features\n",
    "for item in user_features_raw[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b9aca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2', ['releaseDate:Jan-1995', 'genre:Action', 'genre:Adventure', 'genre:Thriller'])\n",
      "('4', ['releaseDate:Jan-1995', 'genre:Action', 'genre:Comedy', 'genre:Drama'])\n",
      "('17', ['releaseDate:Feb-1996', 'genre:Action', 'genre:Comedy', 'genre:Crime', 'genre:Horror', 'genre:Thriller'])\n",
      "('21', ['releaseDate:Feb-1996', 'genre:Action', 'genre:Adventure', 'genre:Comedy', 'genre:Musical', 'genre:Thriller'])\n",
      "('22', ['releaseDate:Feb-1996', 'genre:Action', 'genre:Drama', 'genre:War'])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a defaultdict to hold item features\n",
    "item_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Read data and build item features dictionary\n",
    "# Modified version to handle multiple genres\n",
    "def load_feature(file_path, feature_name):\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            itemId = row['movieId']\n",
    "            value = row[feature_name]\n",
    "            if feature_name == 'genreDesc':\n",
    "                item_data[itemId]['genre'].append(value)\n",
    "            else:\n",
    "                item_data[itemId][feature_name] = value\n",
    "\n",
    "# Load each feature file\n",
    "load_feature(PATH + 'releaseRel.csv', 'releaseDate')\n",
    "load_feature(PATH + 'genreRel.csv', 'genreDesc')\n",
    "\n",
    "# Build item features list\n",
    "item_features_raw = [\n",
    "    (\n",
    "        itemId,\n",
    "        [f'releaseDate:{data[\"releaseDate\"]}'] +\n",
    "        [f'genre:{genre}' for genre in data['genre']]\n",
    "    )\n",
    "    for itemId, data in item_data.items()\n",
    "]\n",
    "\n",
    "# Display first 5 item features\n",
    "for item in item_features_raw[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6969351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items on test set: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['159', '458', '679', '128', '658']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load test item IDs from the json file saved\n",
    "# previously from Knowledge Graph Method\n",
    "with open('../experiments/test_ids.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "# Extract test item IDs as integers\n",
    "test_item_ids = [item['movieId'] for item in data]\n",
    "print(f\"items on test set: {len(test_item_ids)}\")\n",
    "display(test_item_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ddacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets for later evaluation\n",
    "# with LightFM the same way as done in Knowledge Graph Method\n",
    "\n",
    "# Interaction data for training (excluding test items)\n",
    "train_interactions_df = interaction_data[\n",
    "    ~interaction_data['item_id'].astype(str).isin(test_item_ids)]\n",
    "# Interaction data for testing (only test items)\n",
    "test_interactions_df = interaction_data[\n",
    "    interaction_data['item_id'].astype(str).isin(test_item_ids)]\n",
    "\n",
    "# Item side features for training (only users in train interactions)\n",
    "train_item_features = [item for item in item_features_raw if item[0] not in test_item_ids]\n",
    "# Item side features for testing (only testing items)\n",
    "test_item_features = [item for item in item_features_raw if item[0] in test_item_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ce4cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lightfm Dataset\n",
    "dataset = Dataset()\n",
    "\n",
    "# All unique user and item ids because LightFM needs all ids\n",
    "# even if some items are only in the test set\n",
    "train_user_ids = interaction_data['user_id'].astype(str).unique()\n",
    "train_item_ids = interaction_data['item_id'].astype(str).unique()\n",
    "\n",
    "# Unique features from user and item features\n",
    "user_feature_set = set(f for _, feats in user_features_raw for f in feats)\n",
    "item_feature_set = set(f for _, feats in train_item_features for f in feats)\n",
    "\n",
    "# Partial fit\n",
    "dataset.fit(\n",
    "    users=train_user_ids,\n",
    "    items=train_item_ids,\n",
    "    user_features=user_feature_set,\n",
    "    item_features=item_feature_set\n",
    ")\n",
    "\n",
    "# Build training matrices considering only training interactions\n",
    "# so its possible to simulate cold-start for items in the test set\n",
    "(interactions, weights) = dataset.build_interactions(\n",
    "    [(str(row['user_id']), str(row['item_id']), row['rating']) for _, row in train_interactions_df.iterrows()]\n",
    ")\n",
    "\n",
    "user_features = dataset.build_user_features(user_features_raw)\n",
    "item_features = dataset.build_item_features(item_features_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed60c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hiperparameter tuning using Optuna\n",
    "# to find the best parameters for LightFM model\n",
    "\n",
    "# Define the objective function for optimization\n",
    "def objective(trial):\n",
    "    # Hiperparâmetros a otimizar\n",
    "    no_components = trial.suggest_int('no_components', 20, 100)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    loss = trial.suggest_categorical('loss', ['logistic'])\n",
    "\n",
    "    # Validação cruzada: média de N splits\n",
    "    n_splits = 3\n",
    "    scores = []\n",
    "\n",
    "    for _ in range(n_splits):\n",
    "        # Split data into train and validation sets\n",
    "        train, valid = random_train_test_split(\n",
    "            interactions, test_percentage=0.2)\n",
    "        \n",
    "        # Instantiate and train the model\n",
    "        model = LightFM(\n",
    "            no_components=no_components,\n",
    "            learning_rate=learning_rate,\n",
    "            loss=loss\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train,\n",
    "            user_features=user_features,\n",
    "            item_features=item_features,\n",
    "            epochs=20,\n",
    "            num_threads=4\n",
    "        )\n",
    "\n",
    "        # Evaluate the model using precision@k\n",
    "        score = precision_at_k(\n",
    "            model,\n",
    "            valid,\n",
    "            k=5,\n",
    "            user_features=user_features,\n",
    "            item_features=item_features\n",
    "        ).mean()\n",
    "        scores.append(score)\n",
    "        \n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e7ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-04 19:46:32,929] A new study created in memory with name: no-name-34784825-d38e-4bb2-94b0-d598456482c7\n",
      "[I 2025-10-04 19:47:23,885] Trial 0 finished with value: 0.021673833951354027 and parameters: {'no_components': 26, 'learning_rate': 0.01043533248341252, 'loss': 'logistic'}. Best is trial 0 with value: 0.021673833951354027.\n",
      "[I 2025-10-04 19:48:32,146] Trial 1 finished with value: 0.022505251690745354 and parameters: {'no_components': 35, 'learning_rate': 0.011415995226630926, 'loss': 'logistic'}. Best is trial 1 with value: 0.022505251690745354.\n",
      "[I 2025-10-04 19:50:14,782] Trial 2 finished with value: 0.02162516862154007 and parameters: {'no_components': 53, 'learning_rate': 0.08816596213080354, 'loss': 'logistic'}. Best is trial 1 with value: 0.022505251690745354.\n",
      "[I 2025-10-04 19:51:51,070] Trial 3 finished with value: 0.03147687390446663 and parameters: {'no_components': 49, 'learning_rate': 0.00016339036078302793, 'loss': 'logistic'}. Best is trial 3 with value: 0.03147687390446663.\n",
      "[I 2025-10-04 19:54:22,524] Trial 4 finished with value: 0.023338429629802704 and parameters: {'no_components': 79, 'learning_rate': 0.013859697848810562, 'loss': 'logistic'}. Best is trial 3 with value: 0.03147687390446663.\n",
      "[I 2025-10-04 19:56:19,392] Trial 5 finished with value: 0.01905685104429722 and parameters: {'no_components': 60, 'learning_rate': 0.0006226073664242249, 'loss': 'logistic'}. Best is trial 3 with value: 0.03147687390446663.\n",
      "[I 2025-10-04 19:58:52,364] Trial 6 finished with value: 0.02528359182178974 and parameters: {'no_components': 79, 'learning_rate': 0.00023900736343560573, 'loss': 'logistic'}. Best is trial 3 with value: 0.03147687390446663.\n",
      "[I 2025-10-04 20:01:43,052] Trial 7 finished with value: 0.022008642554283142 and parameters: {'no_components': 89, 'learning_rate': 0.07355065722926053, 'loss': 'logistic'}. Best is trial 3 with value: 0.03147687390446663.\n",
      "[I 2025-10-04 20:04:13,570] Trial 8 finished with value: 0.02379464916884899 and parameters: {'no_components': 78, 'learning_rate': 0.03890253774995282, 'loss': 'logistic'}. Best is trial 3 with value: 0.03147687390446663.\n",
      "[I 2025-10-04 20:05:24,299] Trial 9 finished with value: 0.021318063139915466 and parameters: {'no_components': 35, 'learning_rate': 0.0006794459537708452, 'loss': 'logistic'}. Best is trial 3 with value: 0.03147687390446663.\n",
      "[I 2025-10-04 20:07:12,384] Trial 10 finished with value: 0.03335833176970482 and parameters: {'no_components': 55, 'learning_rate': 0.00010439387795796535, 'loss': 'logistic'}. Best is trial 10 with value: 0.03335833176970482.\n",
      "[I 2025-10-04 20:08:56,540] Trial 11 finished with value: 0.03116007335484028 and parameters: {'no_components': 53, 'learning_rate': 0.00011698547360317891, 'loss': 'logistic'}. Best is trial 10 with value: 0.03335833176970482.\n",
      "[I 2025-10-04 20:10:57,269] Trial 12 finished with value: 0.03236587345600128 and parameters: {'no_components': 61, 'learning_rate': 0.00010100773509873045, 'loss': 'logistic'}. Best is trial 10 with value: 0.03335833176970482.\n",
      "[I 2025-10-04 20:13:10,546] Trial 13 finished with value: 0.02292993664741516 and parameters: {'no_components': 69, 'learning_rate': 0.0012657541575926898, 'loss': 'logistic'}. Best is trial 10 with value: 0.03335833176970482.\n",
      "[I 2025-10-04 20:14:35,571] Trial 14 finished with value: 0.020142991095781326 and parameters: {'no_components': 43, 'learning_rate': 0.00034529014526891966, 'loss': 'logistic'}. Best is trial 10 with value: 0.03335833176970482.\n",
      "[I 2025-10-04 20:16:43,038] Trial 15 finished with value: 0.020007070153951645 and parameters: {'no_components': 66, 'learning_rate': 0.001775634093900221, 'loss': 'logistic'}. Best is trial 10 with value: 0.03335833176970482.\n",
      "[I 2025-10-04 20:19:52,928] Trial 16 finished with value: 0.03424060344696045 and parameters: {'no_components': 98, 'learning_rate': 0.0001052591624645766, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:23:02,390] Trial 17 finished with value: 0.016939209774136543 and parameters: {'no_components': 99, 'learning_rate': 0.003847322256519973, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:26:17,202] Trial 18 finished with value: 0.021239394322037697 and parameters: {'no_components': 99, 'learning_rate': 0.0003142395118915235, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:29:09,985] Trial 19 finished with value: 0.020254461094737053 and parameters: {'no_components': 90, 'learning_rate': 0.0005131642491195439, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:31:23,159] Trial 20 finished with value: 0.014026738703250885 and parameters: {'no_components': 70, 'learning_rate': 0.00295463746795163, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:33:21,758] Trial 21 finished with value: 0.03221297636628151 and parameters: {'no_components': 62, 'learning_rate': 0.00010094799988293595, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:34:44,800] Trial 22 finished with value: 0.025241142138838768 and parameters: {'no_components': 43, 'learning_rate': 0.0002206964465944983, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:37:31,905] Trial 23 finished with value: 0.032495807856321335 and parameters: {'no_components': 88, 'learning_rate': 0.0001544334331434319, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:40:28,960] Trial 24 finished with value: 0.031091555953025818 and parameters: {'no_components': 90, 'learning_rate': 0.00017138102369461332, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:43:07,003] Trial 25 finished with value: 0.021197384223341942 and parameters: {'no_components': 84, 'learning_rate': 0.0011112893956496384, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:46:05,985] Trial 26 finished with value: 0.023582106456160545 and parameters: {'no_components': 96, 'learning_rate': 0.0003905822824467908, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:48:24,944] Trial 27 finished with value: 0.02774173766374588 and parameters: {'no_components': 74, 'learning_rate': 0.00020307878329977726, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:51:19,843] Trial 28 finished with value: 0.02403542958199978 and parameters: {'no_components': 94, 'learning_rate': 0.0009017395949026968, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:52:04,963] Trial 29 finished with value: 0.020976638421416283 and parameters: {'no_components': 23, 'learning_rate': 0.006011486865107802, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:54:41,501] Trial 30 finished with value: 0.03164665028452873 and parameters: {'no_components': 83, 'learning_rate': 0.00013861271328675917, 'loss': 'logistic'}. Best is trial 16 with value: 0.03424060344696045.\n",
      "[I 2025-10-04 20:56:12,211] Trial 31 finished with value: 0.03594325855374336 and parameters: {'no_components': 47, 'learning_rate': 0.00010221614281338545, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 20:57:17,905] Trial 32 finished with value: 0.023707836866378784 and parameters: {'no_components': 34, 'learning_rate': 0.0002502489377202053, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 20:59:01,091] Trial 33 finished with value: 0.034672580659389496 and parameters: {'no_components': 54, 'learning_rate': 0.00016571523851721052, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:00:45,710] Trial 34 finished with value: 0.022481491789221764 and parameters: {'no_components': 55, 'learning_rate': 0.00042193508449734826, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:02:08,603] Trial 35 finished with value: 0.029987728223204613 and parameters: {'no_components': 43, 'learning_rate': 0.0001517688913140487, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:03:40,093] Trial 36 finished with value: 0.03431619331240654 and parameters: {'no_components': 48, 'learning_rate': 0.00010038507041727764, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:05:11,283] Trial 37 finished with value: 0.020836912095546722 and parameters: {'no_components': 48, 'learning_rate': 0.000292645981952676, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:06:14,851] Trial 38 finished with value: 0.02252035029232502 and parameters: {'no_components': 33, 'learning_rate': 0.021774910589185178, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:07:46,537] Trial 39 finished with value: 0.02905680239200592 and parameters: {'no_components': 48, 'learning_rate': 0.00019780181236717272, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:09:03,369] Trial 40 finished with value: 0.01845850609242916 and parameters: {'no_components': 40, 'learning_rate': 0.0006179502029645774, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:10:49,850] Trial 41 finished with value: 0.03428078815340996 and parameters: {'no_components': 56, 'learning_rate': 0.00010816772029689668, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:12:38,001] Trial 42 finished with value: 0.03137607499957085 and parameters: {'no_components': 57, 'learning_rate': 0.00014312331444408968, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:14:12,508] Trial 43 finished with value: 0.024858815595507622 and parameters: {'no_components': 50, 'learning_rate': 0.0002200691724010567, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:15:09,474] Trial 44 finished with value: 0.03031350113451481 and parameters: {'no_components': 29, 'learning_rate': 0.00012564736235715516, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:16:59,409] Trial 45 finished with value: 0.031866710633039474 and parameters: {'no_components': 58, 'learning_rate': 0.00010865667481029271, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:18:35,964] Trial 46 finished with value: 0.030224673449993134 and parameters: {'no_components': 51, 'learning_rate': 0.0001747164208701744, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:20:04,105] Trial 47 finished with value: 0.020870210602879524 and parameters: {'no_components': 46, 'learning_rate': 0.0002491788573923595, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:22:04,519] Trial 48 finished with value: 0.01958118937909603 and parameters: {'no_components': 64, 'learning_rate': 0.00046814245163187486, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n",
      "[I 2025-10-04 21:23:20,381] Trial 49 finished with value: 0.020745158195495605 and parameters: {'no_components': 40, 'learning_rate': 0.00029215745455310515, 'loss': 'logistic'}. Best is trial 31 with value: 0.03594325855374336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'no_components': 47, 'learning_rate': 0.00010221614281338545, 'loss': 'logistic'}\n"
     ]
    }
   ],
   "source": [
    "# Run the optimization with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Save the best parameters to a JSON file\n",
    "os.makedirs(\"../experiments\", exist_ok=True)\n",
    "with open(\"../experiments/lightfm_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"best_params\": study.best_params,\n",
    "            \"best_precision_at_5\": study.best_value\n",
    "        },\n",
    "        f,\n",
    "        indent=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec48de31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x2dbdcc59990>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best parameters from the JSON file\n",
    "with open(\"../experiments/lightfm_best_params.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    best = json.load(f)\n",
    "best_params = best[\"best_params\"]\n",
    "\n",
    "# Instantiate and train the final model with the best parameters\n",
    "# and interactions from the entire training set\n",
    "final_model = LightFM(\n",
    "    no_components=best_params[\"no_components\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    loss=best_params[\"loss\"]\n",
    ")\n",
    "final_model.fit(\n",
    "    interactions,\n",
    "    user_features=user_features,\n",
    "    item_features=item_features,\n",
    "    epochs=20,\n",
    "    num_threads=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbdc4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the 50 most relevant users for each item in the test set\n",
    "\n",
    "# Load all the user IDs\n",
    "user_ids = list(train_user_ids)\n",
    "\n",
    "# Build the item features matrix for the test set\n",
    "test_item_features_matrix = dataset.build_item_features(test_item_features)\n",
    "\n",
    "# Generate recommendations for each test item\n",
    "# getting top 50 users for each item so its possible to compare\n",
    "# with Knowledge Graph Method results on k = 10, 20 and 50\n",
    "top_k = 50\n",
    "recommendations = {}\n",
    "\n",
    "# Mapeamento reverso do índice interno para o ID real do usuário\n",
    "user_id_map = {v: k for k, v in dataset.mapping()[0].items()}\n",
    "\n",
    "for item_id in test_item_ids:\n",
    "    # Índice interno do item de teste\n",
    "    item_internal_idx = dataset.mapping()[2][item_id]\n",
    "    # Score para todos os usuários para este item\n",
    "    scores = final_model.predict(\n",
    "        user_ids=np.arange(len(user_ids)),\n",
    "        item_ids=np.repeat(item_internal_idx, len(user_ids)),\n",
    "        user_features=user_features,\n",
    "        item_features=test_item_features_matrix\n",
    "    )\n",
    "    # Top 50 usuários (índices ordenados por score decrescente)\n",
    "    top_users_idx = np.argsort(-scores)[:top_k]\n",
    "    # IDs reais dos usuários\n",
    "    top_users = [user_ids[i] for i in top_users_idx]\n",
    "    recommendations[item_id] = top_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f7fc643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.1667\n",
      "NDCG@10: 0.1856\n",
      "Precision@20: 0.1733\n",
      "NDCG@20: 0.1815\n",
      "Precision@50: 0.2160\n",
      "NDCG@50: 0.2099\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision@k and ndcg@k for k = 10, 20, 50\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    hits = sum([1 for user in recommended_k if user in relevant_set])\n",
    "    return hits / k\n",
    "\n",
    "def dcg_at_k(recommended, relevant, k):\n",
    "    recommended_k = recommended[:k]\n",
    "    relevant_set = set(relevant)\n",
    "    return sum([1 / np.log2(idx + 2) if user in relevant_set else 0\n",
    "                for idx, user in enumerate(recommended_k)])\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k):\n",
    "    dcg = dcg_at_k(recommended, relevant, k)\n",
    "    ideal_dcg = sum([1 / np.log2(idx + 2) for idx in range(min(len(relevant), k))])\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "ks = [10, 20, 50]\n",
    "precision_scores = {k: [] for k in ks}\n",
    "ndcg_scores = {k: [] for k in ks}\n",
    "\n",
    "# Crie um dicionário: item_id -> lista de usuários relevantes (do test_interactions_df)\n",
    "test_relevant = (\n",
    "    test_interactions_df.groupby('item_id')['user_id']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "for item_id, recommended_users in recommendations.items():\n",
    "    # Converta ambos para string para garantir a comparação correta\n",
    "    relevant_users = [str(u) for u in test_relevant.get(int(item_id), [])]\n",
    "    recommended_users = [str(u) for u in recommended_users]\n",
    "    for k in ks:\n",
    "        prec = precision_at_k(recommended_users, relevant_users, k)\n",
    "        ndcg = ndcg_at_k(recommended_users, relevant_users, k)\n",
    "        precision_scores[k].append(prec)\n",
    "        ndcg_scores[k].append(ndcg)\n",
    "\n",
    "# Média para cada k\n",
    "for k in ks:\n",
    "    print(f\"Precision@{k}: {np.mean(precision_scores[k]):.4f}\")\n",
    "    print(f\"NDCG@{k}: {np.mean(ndcg_scores[k]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8bcd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results to a JSON file\n",
    "results = {\n",
    "    f\"precision@{k}\": round(float(np.mean(precision_scores[k])), 2) for k in ks\n",
    "}\n",
    "results.update({\n",
    "    f\"ndcg@{k}\": round(float(np.mean(ndcg_scores[k])), 2) for k in ks\n",
    "})\n",
    "\n",
    "with open(\"../experiments/lightfm_final_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
